{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed',)).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "灞\n",
      "879\n",
      "261\n",
      "转换后的数字化文本 \n",
      " [ 521  808 1042 ...,  496  173  921]\n"
     ]
    }
   ],
   "source": [
    "with open('tanshi.txt','r') as of:\n",
    "    text=of.read()\n",
    "\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c:i for i,c in enumerate(vocab)}\n",
    "\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "print(int_to_vocab[ 1000])\n",
    "print(vocab_to_int['落'])\n",
    "print (vocab_to_int['，']) #换行符号也作为一个文字存在\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n",
    "print ('转换后的数字化文本 \\n',encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 521,  808, 1042,  277,  598,  261,  135,  696,  339,  822,  677,\n",
       "        921, 1304, 1204,  132,  356,  549,  860,  261, 1019,  879,  903,\n",
       "        309, 1285,  921, 1304, 1251,   35,  175, 1114,  647,  261,  646,\n",
       "        157,  965,  819, 1075,  921, 1304,  730,  146,  182,  415,  431,\n",
       "        261, 1228,  380, 1289,  201,  273,  921, 1304, 1226, 1305,  503,\n",
       "        719,  793,  261,  680,  458,  227,  164,  983,  921, 1304,  455,\n",
       "       1220,  823,  465,  385,  261,  655,  598, 1246, 1149, 1134,  921,\n",
       "       1304, 1265,  780,  341, 1011, 1225,  261,  800,  985, 1228,  702,\n",
       "        231,  921, 1304,  922,  630,  655,  598, 1219,  261,  434,  137,\n",
       "       1152])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#检查前1000个编码\n",
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1334"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#文字的unique个数\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换后的形状 (3, 200)\n",
      "第 1 次切，一共会切分batch数量那么多次,本例为10次\n",
      "x 当前为 \n",
      " [[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "   19  20]\n",
      " [201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218\n",
      "  219 220]\n",
      " [401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418\n",
      "  419 420]]\n",
      "x[:,1:] 当前为 \n",
      " [[  2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n",
      "   20]\n",
      " [202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219\n",
      "  220]\n",
      " [402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419\n",
      "  420]]\n",
      "x[:,0] 当前为 [  1 201 401]\n",
      "y 转换后为 \n",
      " [[  2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n",
      "   20   1]\n",
      " [202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219\n",
      "  220 201]\n",
      " [402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419\n",
      "  420 401]]\n",
      "第 2 次切，一共会切分batch数量那么多次,本例为10次\n",
      "x 当前为 \n",
      " [[ 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
      "   39  40]\n",
      " [221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238\n",
      "  239 240]\n",
      " [421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438\n",
      "  439 440]]\n",
      "x[:,1:] 当前为 \n",
      " [[ 22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39\n",
      "   40]\n",
      " [222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239\n",
      "  240]\n",
      " [422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439\n",
      "  440]]\n",
      "x[:,0] 当前为 [ 21 221 421]\n",
      "y 转换后为 \n",
      " [[ 22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39\n",
      "   40  21]\n",
      " [222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239\n",
      "  240 221]\n",
      " [422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439\n",
      "  440 421]]\n",
      "第 3 次切，一共会切分batch数量那么多次,本例为10次\n",
      "x 当前为 \n",
      " [[ 41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
      "   59  60]\n",
      " [241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258\n",
      "  259 260]\n",
      " [441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458\n",
      "  459 460]]\n",
      "x[:,1:] 当前为 \n",
      " [[ 42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
      "   60]\n",
      " [242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259\n",
      "  260]\n",
      " [442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459\n",
      "  460]]\n",
      "x[:,0] 当前为 [ 41 241 441]\n",
      "y 转换后为 \n",
      " [[ 42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
      "   60  41]\n",
      " [242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259\n",
      "  260 241]\n",
      " [442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459\n",
      "  460 441]]\n",
      "第 4 次切，一共会切分batch数量那么多次,本例为10次\n",
      "x 当前为 \n",
      " [[ 61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
      "   79  80]\n",
      " [261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278\n",
      "  279 280]\n",
      " [461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478\n",
      "  479 480]]\n",
      "x[:,1:] 当前为 \n",
      " [[ 62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79\n",
      "   80]\n",
      " [262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279\n",
      "  280]\n",
      " [462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479\n",
      "  480]]\n",
      "x[:,0] 当前为 [ 61 261 461]\n",
      "y 转换后为 \n",
      " [[ 62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79\n",
      "   80  61]\n",
      " [262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279\n",
      "  280 261]\n",
      " [462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479\n",
      "  480 461]]\n",
      "第 5 次切，一共会切分batch数量那么多次,本例为10次\n",
      "x 当前为 \n",
      " [[ 81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
      "   99 100]\n",
      " [281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298\n",
      "  299 300]\n",
      " [481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498\n",
      "  499 500]]\n",
      "x[:,1:] 当前为 \n",
      " [[ 82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99\n",
      "  100]\n",
      " [282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299\n",
      "  300]\n",
      " [482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499\n",
      "  500]]\n",
      "x[:,0] 当前为 [ 81 281 481]\n",
      "y 转换后为 \n",
      " [[ 82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99\n",
      "  100  81]\n",
      " [282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299\n",
      "  300 281]\n",
      " [482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499\n",
      "  500 481]]\n",
      "第 6 次切，一共会切分batch数量那么多次,本例为10次\n",
      "x 当前为 \n",
      " [[101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
      "  119 120]\n",
      " [301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318\n",
      "  319 320]\n",
      " [501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518\n",
      "  519 520]]\n",
      "x[:,1:] 当前为 \n",
      " [[102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
      "  120]\n",
      " [302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319\n",
      "  320]\n",
      " [502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519\n",
      "  520]]\n",
      "x[:,0] 当前为 [101 301 501]\n",
      "y 转换后为 \n",
      " [[102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
      "  120 101]\n",
      " [302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319\n",
      "  320 301]\n",
      " [502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519\n",
      "  520 501]]\n",
      "第 7 次切，一共会切分batch数量那么多次,本例为10次\n",
      "x 当前为 \n",
      " [[121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138\n",
      "  139 140]\n",
      " [321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338\n",
      "  339 340]\n",
      " [521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538\n",
      "  539 540]]\n",
      "x[:,1:] 当前为 \n",
      " [[122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139\n",
      "  140]\n",
      " [322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339\n",
      "  340]\n",
      " [522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      "  540]]\n",
      "x[:,0] 当前为 [121 321 521]\n",
      "y 转换后为 \n",
      " [[122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139\n",
      "  140 121]\n",
      " [322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339\n",
      "  340 321]\n",
      " [522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      "  540 521]]\n",
      "第 8 次切，一共会切分batch数量那么多次,本例为10次\n",
      "x 当前为 \n",
      " [[141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158\n",
      "  159 160]\n",
      " [341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358\n",
      "  359 360]\n",
      " [541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558\n",
      "  559 560]]\n",
      "x[:,1:] 当前为 \n",
      " [[142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159\n",
      "  160]\n",
      " [342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      "  360]\n",
      " [542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559\n",
      "  560]]\n",
      "x[:,0] 当前为 [141 341 541]\n",
      "y 转换后为 \n",
      " [[142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159\n",
      "  160 141]\n",
      " [342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      "  360 341]\n",
      " [542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559\n",
      "  560 541]]\n",
      "第 9 次切，一共会切分batch数量那么多次,本例为10次\n",
      "x 当前为 \n",
      " [[161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178\n",
      "  179 180]\n",
      " [361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378\n",
      "  379 380]\n",
      " [561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578\n",
      "  579 580]]\n",
      "x[:,1:] 当前为 \n",
      " [[162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      "  180]\n",
      " [362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379\n",
      "  380]\n",
      " [562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579\n",
      "  580]]\n",
      "x[:,0] 当前为 [161 361 561]\n",
      "y 转换后为 \n",
      " [[162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      "  180 161]\n",
      " [362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379\n",
      "  380 361]\n",
      " [562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579\n",
      "  580 561]]\n",
      "第 10 次切，一共会切分batch数量那么多次,本例为10次\n",
      "x 当前为 \n",
      " [[181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
      "  199 200]\n",
      " [381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398\n",
      "  399 400]\n",
      " [581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598\n",
      "  599 600]]\n",
      "x[:,1:] 当前为 \n",
      " [[182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199\n",
      "  200]\n",
      " [382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399\n",
      "  400]\n",
      " [582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599\n",
      "  600]]\n",
      "x[:,0] 当前为 [181 381 581]\n",
      "y 转换后为 \n",
      " [[182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199\n",
      "  200 181]\n",
      " [382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399\n",
      "  400 381]\n",
      " [582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599\n",
      "  600 581]]\n"
     ]
    }
   ],
   "source": [
    "#train data deal wtih\n",
    "import numpy as np\n",
    "\n",
    "arr=range(1,601)\n",
    "\n",
    "arr=np.array(arr)\n",
    "\n",
    "n_seqs=3\n",
    "\n",
    "n_steps=20\n",
    "\n",
    "characters_per_batch = n_seqs*n_steps  #60\n",
    "\n",
    "n_batches = len(arr)//characters_per_batch #10批\n",
    "\n",
    "arr = arr[:n_batches * characters_per_batch] #1:600\n",
    "\n",
    "\n",
    "arr = arr.reshape(n_seqs,-1)\n",
    "\n",
    "print('转换后的形状',arr.shape)\n",
    "\n",
    "i=0\n",
    "\n",
    "for n in range(0,arr.shape[1],n_steps): \n",
    "    i=i+1\n",
    "    print('第',i,'次切，一共会切分batch数量那么多次,本例为10次')\n",
    "    \n",
    "    x = arr[:, n:n+n_steps]\n",
    "    print('x 当前为 \\n',x)                      # 切出的x\n",
    "    print('x[:,1:] 当前为 \\n' ,x[:,1:])         #切出的x除去第一列的后面4列，作为y的前面4列\n",
    "    print('x[:,0] 当前为' ,x[:,0])              #x的第一列，将来作为y的最后一列\n",
    "    \n",
    "    \n",
    "    # 对应label输出, shift 一次。也就是知道x=1234，设置y=2341\n",
    "    y = np.zeros_like(x)\n",
    "    y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "    print('y 转换后为 \\n',y)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#利用yield 数据生成器 constructor,供应数据\n",
    "def get_batches(arr,n_seqs,n_steps):\n",
    "    \n",
    "    characters_per_batch = n_seqs * n_steps\n",
    "    n_batches = len(arr)//characters_per_batch\n",
    "    \n",
    "    arr = arr[:n_batches * characters_per_batch]\n",
    "    \n",
    "    arr = arr.reshape((n_seqs,-1))\n",
    "    \n",
    "    for n in range(0,arr.shape[1],n_steps):\n",
    "        \n",
    "        x = arr[:, n:n+n_steps]\n",
    "        \n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:,0]\n",
    "        \n",
    "        yield x ,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded,3,20)\n",
    "\n",
    "#get_barches 函数中的\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x \n",
      "  [[ 521  808 1042  277  598  261  135  696  339  822  677  921 1304 1204\n",
      "   132  356  549  860  261 1019]\n",
      " [1038  279   81  261  493 1133  473  349  146  921 1304  612   87  415\n",
      "  1245 1090  261    0  201  647]\n",
      " [ 178  879  905  197   57  261 1283  766  774  915  449  921 1304   42\n",
      "  1128  371  428  339  261 1180]]\n",
      "y \n",
      "  [[ 808 1042  277  598  261  135  696  339  822  677  921 1304 1204  132\n",
      "   356  549  860  261 1019  521]\n",
      " [ 279   81  261  493 1133  473  349  146  921 1304  612   87  415 1245\n",
      "  1090  261    0  201  647 1038]\n",
      " [ 879  905  197   57  261 1283  766  774  915  449  921 1304   42 1128\n",
      "   371  428  339  261 1180  178]]\n"
     ]
    }
   ],
   "source": [
    "print('x \\n ',x)\n",
    "print('y \\n ',y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size,num_steps], name='targets')\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    print('build_inputs 函数中的input',input)\n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size,num_layers, batch_size,keep_prob):\n",
    "    \n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell( [drop] * num_layers )\n",
    "    \n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    print('build LSTM函数中传入的batch_size',batch_size)\n",
    "    \n",
    "    return cell, initial_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    \n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    print('build_output 函数中的seq_output 是否为10列 60行 ：',seq_output) #需要删除\n",
    "    x = tf.reshape(seq_output,[-1, in_size])\n",
    "    print('build_output 函数中的 reshape seqput ', x) #需要删除\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    #连接RNN的结果到 softmax layer \n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size,out_size),stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "        \n",
    "        #logits应该是 (M * N ) * (字典个数列)\n",
    "    logits = tf.matmul(x,softmax_w) + softmax_b\n",
    "    print('build_output 函数中的logit 是否是字典个数:',logits)\n",
    "    # Use softmax to get the probabilities for predicted charactersprint('b')\n",
    "        #输出预测\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    print('build_output 函数中的softmax_out',out)\n",
    "    return out, logits\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    \n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "     # Softmax cross entropy loss\n",
    "    #获得loss 以交叉熵作为损失\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    \n",
    "    tvars = tf.trainable_variables()\n",
    "    \n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss,tvars), grad_clip)\n",
    "    \n",
    "    #设置采用AdamOptimizer优化器\n",
    "    train_op = tf.train.AdadeltaOptimizer(learning_rate)\n",
    "    \n",
    "    optimizer = train_op.apply_gradients(zip(grads,tvars))\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    #If before you were using: MultiRNNCell([BasicLSTMCell(...)] * num_layers), \n",
    "    #change to: MultiRNNCell([BasicLSTMCell(...) for _ in range(num_layers)]). \n",
    "    #num_classes对应前面输出结构的大小。我们这里是字典的数量245\n",
    "    def __init__(self, num_classes, batch_size=3, num_steps=20, \n",
    "                       lstm_size=10, num_layers=5, learning_rate=0.01, \n",
    "                       grad_clip=5, sampling=False):\n",
    "        '''\n",
    "        参数说明及默认设置说明：\n",
    "        num_classes 为分类，如果是情感分析就是2类。我们这里由于需要输出文章，相对更复杂一些，类似对文章的自编码。\n",
    "        batch_size=4 ，定义单个batch 4个序列\n",
    "        num_steps=20,定义一个序列默认20个字符\n",
    "        lstm_size=5 ，定义单个LSTM Cell 中有5个unit。\n",
    "        num_layers=5 ，定义LSTM Cell 有5层\n",
    "        learning_rate=0.01，定义学习率，注意0.001这样更细的学习率可能更好，此处设置为0.01主要是为了个人电脑CPU跟快收敛一些。\n",
    "        sampling=Fasle帮助我们做一些基础采样学习，或者创建基准用\n",
    "        sampling==True 表示 batch_size, num_steps = 1, 1 ,sampling==False 设置为batch_size, num_steps = batch_size, num_steps\n",
    "        \n",
    "        '''\n",
    "        \n",
    "    \n",
    "        # 当采样Sampling == True, 一次处理一个字符\n",
    "\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "        \n",
    "        #复位default graph\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # 通过build_inputs返回 3个 placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # 创建LSTM cell 返回LSTM Cell和初始状态\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        ### 处理数据\n",
    "        #  对输入做 one-hot encode \n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "       \n",
    "        \n",
    "        # Run each sequence step through the RNN and collect the outputs\n",
    "        #运行每一个序列step，\n",
    "        #传入1 lstm cell对象以及 2 输入数据x（一个batch中的4个序列） 和 3 LSTM最后一层的cell states 初始状态\n",
    "        #返回输出和state\n",
    "        print(x_one_hot)\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # 获得 softmax predictions 和 logits\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # 定义损失函数 和带梯度修剪的梯度下降优化器 \n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#设置训练超参数\n",
    "batch_size = 3       \n",
    "num_steps = 20        \n",
    "lstm_size = 10        \n",
    "num_layers = 5         \n",
    "learning_rate = 0.01   \n",
    "keep_prob = 0.5   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_inputs 函数中的input <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x0000000004BB3860>>\n",
      "build LSTM函数中传入的batch_size 3\n",
      "Tensor(\"one_hot:0\", shape=(3, 20, 1334), dtype=float32)\n",
      "build_output 函数中的seq_output 是否为10列 60行 ： Tensor(\"concat:0\", shape=(3, 20, 10), dtype=float32)\n",
      "build_output 函数中的 reshape seqput  Tensor(\"Reshape:0\", shape=(60, 10), dtype=float32)\n",
      "build_output 函数中的logit 是否是字典个数: Tensor(\"add:0\", shape=(60, 1334), dtype=float32)\n",
      "build_output 函数中的softmax_out Tensor(\"predictions:0\", shape=(60, 1334), dtype=float32)\n",
      "Epoch: 1/20...  Training Step: 1...  Training loss: 7.1960...  0.1890 sec/batch\n",
      "Epoch: 1/20...  Training Step: 2...  Training loss: 7.1962...  0.0430 sec/batch\n",
      "Epoch: 1/20...  Training Step: 3...  Training loss: 7.1960...  0.0360 sec/batch\n",
      "Epoch: 1/20...  Training Step: 4...  Training loss: 7.1959...  0.0410 sec/batch\n",
      "Epoch: 1/20...  Training Step: 5...  Training loss: 7.1959...  0.0370 sec/batch\n",
      "Epoch: 1/20...  Training Step: 6...  Training loss: 7.1957...  0.0420 sec/batch\n",
      "Epoch: 1/20...  Training Step: 7...  Training loss: 7.1959...  0.0690 sec/batch\n",
      "Epoch: 1/20...  Training Step: 8...  Training loss: 7.1960...  0.0510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 9...  Training loss: 7.1959...  0.0370 sec/batch\n",
      "Epoch: 1/20...  Training Step: 10...  Training loss: 7.1959...  0.0410 sec/batch\n",
      "Epoch: 1/20...  Training Step: 11...  Training loss: 7.1959...  0.0440 sec/batch\n",
      "Epoch: 1/20...  Training Step: 12...  Training loss: 7.1959...  0.0450 sec/batch\n",
      "Epoch: 1/20...  Training Step: 13...  Training loss: 7.1959...  0.0600 sec/batch\n",
      "Epoch: 1/20...  Training Step: 14...  Training loss: 7.1959...  0.0400 sec/batch\n",
      "Epoch: 1/20...  Training Step: 15...  Training loss: 7.1962...  0.0410 sec/batch\n",
      "Epoch: 1/20...  Training Step: 16...  Training loss: 7.1960...  0.0390 sec/batch\n",
      "Epoch: 1/20...  Training Step: 17...  Training loss: 7.1960...  0.0570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 18...  Training loss: 7.1960...  0.0450 sec/batch\n",
      "Epoch: 1/20...  Training Step: 19...  Training loss: 7.1958...  0.0460 sec/batch\n",
      "Epoch: 1/20...  Training Step: 20...  Training loss: 7.1959...  0.0410 sec/batch\n",
      "Epoch: 1/20...  Training Step: 21...  Training loss: 7.1960...  0.0380 sec/batch\n",
      "Epoch: 1/20...  Training Step: 22...  Training loss: 7.1958...  0.0410 sec/batch\n",
      "Epoch: 1/20...  Training Step: 23...  Training loss: 7.1959...  0.0450 sec/batch\n",
      "Epoch: 1/20...  Training Step: 24...  Training loss: 7.1957...  0.0450 sec/batch\n",
      "Epoch: 1/20...  Training Step: 25...  Training loss: 7.1958...  0.0420 sec/batch\n",
      "Epoch: 1/20...  Training Step: 26...  Training loss: 7.1958...  0.0530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 27...  Training loss: 7.1956...  0.0500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 28...  Training loss: 7.1958...  0.0440 sec/batch\n",
      "Epoch: 1/20...  Training Step: 29...  Training loss: 7.1958...  0.0390 sec/batch\n",
      "Epoch: 1/20...  Training Step: 30...  Training loss: 7.1958...  0.0400 sec/batch\n",
      "Epoch: 1/20...  Training Step: 31...  Training loss: 7.1958...  0.0490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 32...  Training loss: 7.1958...  0.0640 sec/batch\n",
      "Epoch: 1/20...  Training Step: 33...  Training loss: 7.1958...  0.0430 sec/batch\n",
      "Epoch: 1/20...  Training Step: 34...  Training loss: 7.1957...  0.0390 sec/batch\n",
      "Epoch: 1/20...  Training Step: 35...  Training loss: 7.1958...  0.0410 sec/batch\n",
      "Epoch: 1/20...  Training Step: 36...  Training loss: 7.1957...  0.0650 sec/batch\n",
      "Epoch: 1/20...  Training Step: 37...  Training loss: 7.1961...  0.0450 sec/batch\n",
      "Epoch: 1/20...  Training Step: 38...  Training loss: 7.1958...  0.0470 sec/batch\n",
      "Epoch: 1/20...  Training Step: 39...  Training loss: 7.1958...  0.0440 sec/batch\n",
      "Epoch: 1/20...  Training Step: 40...  Training loss: 7.1957...  0.0670 sec/batch\n",
      "Epoch: 1/20...  Training Step: 41...  Training loss: 7.1959...  0.0410 sec/batch\n",
      "Epoch: 1/20...  Training Step: 42...  Training loss: 7.1958...  0.0740 sec/batch\n",
      "Epoch: 1/20...  Training Step: 43...  Training loss: 7.1957...  0.0890 sec/batch\n",
      "Epoch: 1/20...  Training Step: 44...  Training loss: 7.1958...  0.0700 sec/batch\n",
      "Epoch: 1/20...  Training Step: 45...  Training loss: 7.1958...  0.0530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 46...  Training loss: 7.1957...  0.0900 sec/batch\n",
      "Epoch: 1/20...  Training Step: 47...  Training loss: 7.1958...  0.0550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 48...  Training loss: 7.1959...  0.0700 sec/batch\n",
      "Epoch: 1/20...  Training Step: 49...  Training loss: 7.1958...  0.0680 sec/batch\n",
      "Epoch: 1/20...  Training Step: 50...  Training loss: 7.1958...  0.0790 sec/batch\n",
      "Epoch: 1/20...  Training Step: 51...  Training loss: 7.1957...  0.0950 sec/batch\n",
      "Epoch: 1/20...  Training Step: 52...  Training loss: 7.1958...  0.0640 sec/batch\n",
      "Epoch: 1/20...  Training Step: 53...  Training loss: 7.1956...  0.0440 sec/batch\n",
      "Epoch: 1/20...  Training Step: 54...  Training loss: 7.1957...  0.0720 sec/batch\n",
      "Epoch: 1/20...  Training Step: 55...  Training loss: 7.1958...  0.0870 sec/batch\n",
      "Epoch: 1/20...  Training Step: 56...  Training loss: 7.1958...  0.0480 sec/batch\n",
      "Epoch: 1/20...  Training Step: 57...  Training loss: 7.1957...  0.0770 sec/batch\n",
      "Epoch: 1/20...  Training Step: 58...  Training loss: 7.1957...  0.0870 sec/batch\n",
      "Epoch: 1/20...  Training Step: 59...  Training loss: 7.1958...  0.0620 sec/batch\n",
      "Epoch: 1/20...  Training Step: 60...  Training loss: 7.1959...  0.0760 sec/batch\n",
      "Epoch: 1/20...  Training Step: 61...  Training loss: 7.1957...  0.0500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 62...  Training loss: 7.1957...  0.0510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 63...  Training loss: 7.1956...  0.0480 sec/batch\n",
      "Epoch: 1/20...  Training Step: 64...  Training loss: 7.1957...  0.0420 sec/batch\n",
      "Epoch: 1/20...  Training Step: 65...  Training loss: 7.1957...  0.0450 sec/batch\n",
      "Epoch: 1/20...  Training Step: 66...  Training loss: 7.1959...  0.0640 sec/batch\n",
      "Epoch: 1/20...  Training Step: 67...  Training loss: 7.1956...  0.0510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 68...  Training loss: 7.1955...  0.0440 sec/batch\n",
      "Epoch: 1/20...  Training Step: 69...  Training loss: 7.1958...  0.0690 sec/batch\n",
      "Epoch: 1/20...  Training Step: 70...  Training loss: 7.1958...  0.0500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 71...  Training loss: 7.1956...  0.0800 sec/batch\n",
      "Epoch: 1/20...  Training Step: 72...  Training loss: 7.1958...  0.0490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 73...  Training loss: 7.1956...  0.0470 sec/batch\n",
      "Epoch: 1/20...  Training Step: 74...  Training loss: 7.1957...  0.0630 sec/batch\n",
      "Epoch: 1/20...  Training Step: 75...  Training loss: 7.1954...  0.0530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 76...  Training loss: 7.1958...  0.0520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 77...  Training loss: 7.1958...  0.0460 sec/batch\n",
      "Epoch: 1/20...  Training Step: 78...  Training loss: 7.1959...  0.0440 sec/batch\n",
      "Epoch: 1/20...  Training Step: 79...  Training loss: 7.1956...  0.0760 sec/batch\n",
      "Epoch: 1/20...  Training Step: 80...  Training loss: 7.1958...  0.0560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 81...  Training loss: 7.1959...  0.0840 sec/batch\n",
      "Epoch: 1/20...  Training Step: 82...  Training loss: 7.1956...  0.0640 sec/batch\n",
      "Epoch: 1/20...  Training Step: 83...  Training loss: 7.1957...  0.0650 sec/batch\n",
      "Epoch: 1/20...  Training Step: 84...  Training loss: 7.1957...  0.0570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 85...  Training loss: 7.1956...  0.0700 sec/batch\n",
      "Epoch: 1/20...  Training Step: 86...  Training loss: 7.1958...  0.0490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 87...  Training loss: 7.1956...  0.1280 sec/batch\n",
      "Epoch: 1/20...  Training Step: 88...  Training loss: 7.1957...  0.0860 sec/batch\n",
      "Epoch: 1/20...  Training Step: 89...  Training loss: 7.1957...  0.0580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 90...  Training loss: 7.1957...  0.0480 sec/batch\n",
      "Epoch: 1/20...  Training Step: 91...  Training loss: 7.1955...  0.0660 sec/batch\n",
      "Epoch: 1/20...  Training Step: 92...  Training loss: 7.1956...  0.0570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 93...  Training loss: 7.1957...  0.0850 sec/batch\n",
      "Epoch: 1/20...  Training Step: 94...  Training loss: 7.1956...  0.0460 sec/batch\n",
      "Epoch: 1/20...  Training Step: 95...  Training loss: 7.1956...  0.0480 sec/batch\n",
      "Epoch: 1/20...  Training Step: 96...  Training loss: 7.1957...  0.0550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 97...  Training loss: 7.1956...  0.0790 sec/batch\n",
      "Epoch: 1/20...  Training Step: 98...  Training loss: 7.1956...  0.0480 sec/batch\n",
      "Epoch: 1/20...  Training Step: 99...  Training loss: 7.1956...  0.0400 sec/batch\n",
      "Epoch: 1/20...  Training Step: 100...  Training loss: 7.1958...  0.0520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 101...  Training loss: 7.1956...  0.0380 sec/batch\n",
      "Epoch: 1/20...  Training Step: 102...  Training loss: 7.1958...  0.0390 sec/batch\n",
      "Epoch: 1/20...  Training Step: 103...  Training loss: 7.1955...  0.0410 sec/batch\n",
      "Epoch: 1/20...  Training Step: 104...  Training loss: 7.1953...  0.0380 sec/batch\n",
      "Epoch: 1/20...  Training Step: 105...  Training loss: 7.1957...  0.0400 sec/batch\n",
      "Epoch: 1/20...  Training Step: 106...  Training loss: 7.1957...  0.0410 sec/batch\n",
      "Epoch: 1/20...  Training Step: 107...  Training loss: 7.1956...  0.0420 sec/batch\n",
      "Epoch: 1/20...  Training Step: 108...  Training loss: 7.1959...  0.0540 sec/batch\n",
      "Epoch: 1/20...  Training Step: 109...  Training loss: 7.1957...  0.0610 sec/batch\n",
      "Epoch: 1/20...  Training Step: 110...  Training loss: 7.1954...  0.0420 sec/batch\n",
      "Epoch: 1/20...  Training Step: 111...  Training loss: 7.1957...  0.0410 sec/batch\n",
      "Epoch: 1/20...  Training Step: 112...  Training loss: 7.1956...  0.0640 sec/batch\n",
      "Epoch: 1/20...  Training Step: 113...  Training loss: 7.1956...  0.0580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 114...  Training loss: 7.1957...  0.0590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 115...  Training loss: 7.1955...  0.0640 sec/batch\n",
      "Epoch: 1/20...  Training Step: 116...  Training loss: 7.1958...  0.0570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 117...  Training loss: 7.1957...  0.0510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 118...  Training loss: 7.1957...  0.0620 sec/batch\n",
      "Epoch: 2/20...  Training Step: 119...  Training loss: 7.1956...  0.0480 sec/batch\n",
      "Epoch: 2/20...  Training Step: 120...  Training loss: 7.1956...  0.0510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 121...  Training loss: 7.1955...  0.0430 sec/batch\n",
      "Epoch: 2/20...  Training Step: 122...  Training loss: 7.1955...  0.0440 sec/batch\n",
      "Epoch: 2/20...  Training Step: 123...  Training loss: 7.1954...  0.0500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 124...  Training loss: 7.1955...  0.0420 sec/batch\n",
      "Epoch: 2/20...  Training Step: 125...  Training loss: 7.1957...  0.0420 sec/batch\n",
      "Epoch: 2/20...  Training Step: 126...  Training loss: 7.1957...  0.0590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 127...  Training loss: 7.1956...  0.0710 sec/batch\n",
      "Epoch: 2/20...  Training Step: 128...  Training loss: 7.1957...  0.0440 sec/batch\n",
      "Epoch: 2/20...  Training Step: 129...  Training loss: 7.1958...  0.0450 sec/batch\n",
      "Epoch: 2/20...  Training Step: 130...  Training loss: 7.1956...  0.0420 sec/batch\n",
      "Epoch: 2/20...  Training Step: 131...  Training loss: 7.1957...  0.0510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 132...  Training loss: 7.1956...  0.0470 sec/batch\n",
      "Epoch: 2/20...  Training Step: 133...  Training loss: 7.1956...  0.0650 sec/batch\n",
      "Epoch: 2/20...  Training Step: 134...  Training loss: 7.1957...  0.0440 sec/batch\n",
      "Epoch: 2/20...  Training Step: 135...  Training loss: 7.1956...  0.0850 sec/batch\n",
      "Epoch: 2/20...  Training Step: 136...  Training loss: 7.1955...  0.0390 sec/batch\n",
      "Epoch: 2/20...  Training Step: 137...  Training loss: 7.1956...  0.0410 sec/batch\n",
      "Epoch: 2/20...  Training Step: 138...  Training loss: 7.1955...  0.0390 sec/batch\n",
      "Epoch: 2/20...  Training Step: 139...  Training loss: 7.1955...  0.0780 sec/batch\n",
      "Epoch: 2/20...  Training Step: 140...  Training loss: 7.1955...  0.0500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 141...  Training loss: 7.1954...  0.0490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 142...  Training loss: 7.1955...  0.0420 sec/batch\n",
      "Epoch: 2/20...  Training Step: 143...  Training loss: 7.1956...  0.0400 sec/batch\n",
      "Epoch: 2/20...  Training Step: 144...  Training loss: 7.1956...  0.0410 sec/batch\n",
      "Epoch: 2/20...  Training Step: 145...  Training loss: 7.1958...  0.0400 sec/batch\n",
      "Epoch: 2/20...  Training Step: 146...  Training loss: 7.1954...  0.0490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 147...  Training loss: 7.1954...  0.0780 sec/batch\n",
      "Epoch: 2/20...  Training Step: 148...  Training loss: 7.1956...  0.0450 sec/batch\n",
      "Epoch: 2/20...  Training Step: 149...  Training loss: 7.1955...  0.0450 sec/batch\n",
      "Epoch: 2/20...  Training Step: 150...  Training loss: 7.1955...  0.0590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 151...  Training loss: 7.1952...  0.0650 sec/batch\n",
      "Epoch: 2/20...  Training Step: 152...  Training loss: 7.1957...  0.0430 sec/batch\n",
      "Epoch: 2/20...  Training Step: 153...  Training loss: 7.1955...  0.0390 sec/batch\n",
      "Epoch: 2/20...  Training Step: 154...  Training loss: 7.1954...  0.0560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 155...  Training loss: 7.1956...  0.0650 sec/batch\n",
      "Epoch: 2/20...  Training Step: 156...  Training loss: 7.1955...  0.0530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 157...  Training loss: 7.1955...  0.0510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 158...  Training loss: 7.1955...  0.0570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 159...  Training loss: 7.1955...  0.0720 sec/batch\n",
      "Epoch: 2/20...  Training Step: 160...  Training loss: 7.1956...  0.0430 sec/batch\n",
      "Epoch: 2/20...  Training Step: 161...  Training loss: 7.1954...  0.0360 sec/batch\n",
      "Epoch: 2/20...  Training Step: 162...  Training loss: 7.1955...  0.0470 sec/batch\n",
      "Epoch: 2/20...  Training Step: 163...  Training loss: 7.1953...  0.0380 sec/batch\n",
      "Epoch: 2/20...  Training Step: 164...  Training loss: 7.1953...  0.0550 sec/batch\n",
      "Epoch: 2/20...  Training Step: 165...  Training loss: 7.1955...  0.0470 sec/batch\n",
      "Epoch: 2/20...  Training Step: 166...  Training loss: 7.1954...  0.0590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 167...  Training loss: 7.1952...  0.0570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 168...  Training loss: 7.1955...  0.0650 sec/batch\n",
      "Epoch: 2/20...  Training Step: 169...  Training loss: 7.1954...  0.0410 sec/batch\n",
      "Epoch: 2/20...  Training Step: 170...  Training loss: 7.1952...  0.0400 sec/batch\n",
      "Epoch: 2/20...  Training Step: 171...  Training loss: 7.1955...  0.0700 sec/batch\n",
      "Epoch: 2/20...  Training Step: 172...  Training loss: 7.1954...  0.0470 sec/batch\n",
      "Epoch: 2/20...  Training Step: 173...  Training loss: 7.1954...  0.0400 sec/batch\n",
      "Epoch: 2/20...  Training Step: 174...  Training loss: 7.1955...  0.0390 sec/batch\n",
      "Epoch: 2/20...  Training Step: 175...  Training loss: 7.1956...  0.0780 sec/batch\n",
      "Epoch: 2/20...  Training Step: 176...  Training loss: 7.1955...  0.0460 sec/batch\n",
      "Epoch: 2/20...  Training Step: 177...  Training loss: 7.1955...  0.0390 sec/batch\n",
      "Epoch: 2/20...  Training Step: 178...  Training loss: 7.1955...  0.0450 sec/batch\n",
      "Epoch: 2/20...  Training Step: 179...  Training loss: 7.1955...  0.0410 sec/batch\n",
      "Epoch: 2/20...  Training Step: 180...  Training loss: 7.1954...  0.0490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 181...  Training loss: 7.1955...  0.0460 sec/batch\n",
      "Epoch: 2/20...  Training Step: 182...  Training loss: 7.1954...  0.0510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 183...  Training loss: 7.1954...  0.0570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 184...  Training loss: 7.1952...  0.0440 sec/batch\n",
      "Epoch: 2/20...  Training Step: 185...  Training loss: 7.1956...  0.0390 sec/batch\n",
      "Epoch: 2/20...  Training Step: 186...  Training loss: 7.1953...  0.0570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 187...  Training loss: 7.1955...  0.0750 sec/batch\n",
      "Epoch: 2/20...  Training Step: 188...  Training loss: 7.1954...  0.0420 sec/batch\n",
      "Epoch: 2/20...  Training Step: 189...  Training loss: 7.1956...  0.0410 sec/batch\n",
      "Epoch: 2/20...  Training Step: 190...  Training loss: 7.1953...  0.0580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 191...  Training loss: 7.1952...  0.0530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 192...  Training loss: 7.1958...  0.0660 sec/batch\n",
      "Epoch: 2/20...  Training Step: 193...  Training loss: 7.1953...  0.0550 sec/batch\n",
      "Epoch: 2/20...  Training Step: 194...  Training loss: 7.1953...  0.0670 sec/batch\n",
      "Epoch: 2/20...  Training Step: 195...  Training loss: 7.1956...  0.0710 sec/batch\n",
      "Epoch: 2/20...  Training Step: 196...  Training loss: 7.1955...  0.0500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 197...  Training loss: 7.1956...  0.0440 sec/batch\n",
      "Epoch: 2/20...  Training Step: 198...  Training loss: 7.1955...  0.0440 sec/batch\n",
      "Epoch: 2/20...  Training Step: 199...  Training loss: 7.1953...  0.0720 sec/batch\n",
      "Epoch: 2/20...  Training Step: 200...  Training loss: 7.1955...  0.0430 sec/batch\n",
      "Epoch: 2/20...  Training Step: 201...  Training loss: 7.1954...  0.0380 sec/batch\n",
      "Epoch: 2/20...  Training Step: 202...  Training loss: 7.1953...  0.0520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 203...  Training loss: 7.1956...  0.0400 sec/batch\n",
      "Epoch: 2/20...  Training Step: 204...  Training loss: 7.1954...  0.0440 sec/batch\n",
      "Epoch: 2/20...  Training Step: 205...  Training loss: 7.1954...  0.0490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 206...  Training loss: 7.1954...  0.0620 sec/batch\n",
      "Epoch: 2/20...  Training Step: 207...  Training loss: 7.1953...  0.0780 sec/batch\n",
      "Epoch: 2/20...  Training Step: 208...  Training loss: 7.1953...  0.0420 sec/batch\n",
      "Epoch: 2/20...  Training Step: 209...  Training loss: 7.1953...  0.0380 sec/batch\n",
      "Epoch: 2/20...  Training Step: 210...  Training loss: 7.1952...  0.0430 sec/batch\n",
      "Epoch: 2/20...  Training Step: 211...  Training loss: 7.1953...  0.0570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 212...  Training loss: 7.1953...  0.0580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 213...  Training loss: 7.1953...  0.0450 sec/batch\n",
      "Epoch: 2/20...  Training Step: 214...  Training loss: 7.1953...  0.0460 sec/batch\n",
      "Epoch: 2/20...  Training Step: 215...  Training loss: 7.1954...  0.0670 sec/batch\n",
      "Epoch: 2/20...  Training Step: 216...  Training loss: 7.1952...  0.0410 sec/batch\n",
      "Epoch: 2/20...  Training Step: 217...  Training loss: 7.1954...  0.0430 sec/batch\n",
      "Epoch: 2/20...  Training Step: 218...  Training loss: 7.1953...  0.0380 sec/batch\n",
      "Epoch: 2/20...  Training Step: 219...  Training loss: 7.1954...  0.0650 sec/batch\n",
      "Epoch: 2/20...  Training Step: 220...  Training loss: 7.1951...  0.0420 sec/batch\n",
      "Epoch: 2/20...  Training Step: 221...  Training loss: 7.1953...  0.0490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 222...  Training loss: 7.1954...  0.0390 sec/batch\n",
      "Epoch: 2/20...  Training Step: 223...  Training loss: 7.1953...  0.0490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 224...  Training loss: 7.1957...  0.0420 sec/batch\n",
      "Epoch: 2/20...  Training Step: 225...  Training loss: 7.1955...  0.0510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 226...  Training loss: 7.1954...  0.0670 sec/batch\n",
      "Epoch: 2/20...  Training Step: 227...  Training loss: 7.1950...  0.0700 sec/batch\n",
      "Epoch: 2/20...  Training Step: 228...  Training loss: 7.1954...  0.0430 sec/batch\n",
      "Epoch: 2/20...  Training Step: 229...  Training loss: 7.1951...  0.0460 sec/batch\n",
      "Epoch: 2/20...  Training Step: 230...  Training loss: 7.1950...  0.0510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 231...  Training loss: 7.1954...  0.0750 sec/batch\n",
      "Epoch: 2/20...  Training Step: 232...  Training loss: 7.1950...  0.0490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 233...  Training loss: 7.1953...  0.0460 sec/batch\n",
      "Epoch: 2/20...  Training Step: 234...  Training loss: 7.1955...  0.0690 sec/batch\n",
      "Epoch: 3/20...  Training Step: 235...  Training loss: 7.1955...  0.0800 sec/batch\n",
      "Epoch: 3/20...  Training Step: 236...  Training loss: 7.1954...  0.0500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 237...  Training loss: 7.1954...  0.0420 sec/batch\n",
      "Epoch: 3/20...  Training Step: 238...  Training loss: 7.1949...  0.0680 sec/batch\n",
      "Epoch: 3/20...  Training Step: 239...  Training loss: 7.1953...  0.0630 sec/batch\n",
      "Epoch: 3/20...  Training Step: 240...  Training loss: 7.1954...  0.0410 sec/batch\n",
      "Epoch: 3/20...  Training Step: 241...  Training loss: 7.1953...  0.0470 sec/batch\n",
      "Epoch: 3/20...  Training Step: 242...  Training loss: 7.1953...  0.0860 sec/batch\n",
      "Epoch: 3/20...  Training Step: 243...  Training loss: 7.1955...  0.0630 sec/batch\n",
      "Epoch: 3/20...  Training Step: 244...  Training loss: 7.1953...  0.0830 sec/batch\n",
      "Epoch: 3/20...  Training Step: 245...  Training loss: 7.1956...  0.0860 sec/batch\n",
      "Epoch: 3/20...  Training Step: 246...  Training loss: 7.1954...  0.0530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 247...  Training loss: 7.1953...  0.0840 sec/batch\n",
      "Epoch: 3/20...  Training Step: 248...  Training loss: 7.1952...  0.0480 sec/batch\n",
      "Epoch: 3/20...  Training Step: 249...  Training loss: 7.1954...  0.0610 sec/batch\n",
      "Epoch: 3/20...  Training Step: 250...  Training loss: 7.1954...  0.0610 sec/batch\n",
      "Epoch: 3/20...  Training Step: 251...  Training loss: 7.1952...  0.0490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 252...  Training loss: 7.1953...  0.0630 sec/batch\n",
      "Epoch: 3/20...  Training Step: 253...  Training loss: 7.1952...  0.0470 sec/batch\n",
      "Epoch: 3/20...  Training Step: 254...  Training loss: 7.1952...  0.0400 sec/batch\n",
      "Epoch: 3/20...  Training Step: 255...  Training loss: 7.1954...  0.0820 sec/batch\n",
      "Epoch: 3/20...  Training Step: 256...  Training loss: 7.1951...  0.0550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 257...  Training loss: 7.1952...  0.0700 sec/batch\n",
      "Epoch: 3/20...  Training Step: 258...  Training loss: 7.1951...  0.0470 sec/batch\n",
      "Epoch: 3/20...  Training Step: 259...  Training loss: 7.1952...  0.0700 sec/batch\n",
      "Epoch: 3/20...  Training Step: 260...  Training loss: 7.1953...  0.0540 sec/batch\n",
      "Epoch: 3/20...  Training Step: 261...  Training loss: 7.1950...  0.0570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 262...  Training loss: 7.1951...  0.0660 sec/batch\n",
      "Epoch: 3/20...  Training Step: 263...  Training loss: 7.1951...  0.0760 sec/batch\n",
      "Epoch: 3/20...  Training Step: 264...  Training loss: 7.1950...  0.0920 sec/batch\n",
      "Epoch: 3/20...  Training Step: 265...  Training loss: 7.1953...  0.0680 sec/batch\n",
      "Epoch: 3/20...  Training Step: 266...  Training loss: 7.1950...  0.0900 sec/batch\n",
      "Epoch: 3/20...  Training Step: 267...  Training loss: 7.1955...  0.0400 sec/batch\n",
      "Epoch: 3/20...  Training Step: 268...  Training loss: 7.1952...  0.0880 sec/batch\n",
      "Epoch: 3/20...  Training Step: 269...  Training loss: 7.1954...  0.0570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 270...  Training loss: 7.1950...  0.0420 sec/batch\n",
      "Epoch: 3/20...  Training Step: 271...  Training loss: 7.1954...  0.0400 sec/batch\n",
      "Epoch: 3/20...  Training Step: 272...  Training loss: 7.1953...  0.0400 sec/batch\n",
      "Epoch: 3/20...  Training Step: 273...  Training loss: 7.1952...  0.0630 sec/batch\n",
      "Epoch: 3/20...  Training Step: 274...  Training loss: 7.1950...  0.0520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 275...  Training loss: 7.1952...  0.0690 sec/batch\n",
      "Epoch: 3/20...  Training Step: 276...  Training loss: 7.1953...  0.0460 sec/batch\n",
      "Epoch: 3/20...  Training Step: 277...  Training loss: 7.1949...  0.0500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 278...  Training loss: 7.1953...  0.0630 sec/batch\n",
      "Epoch: 3/20...  Training Step: 279...  Training loss: 7.1954...  0.0620 sec/batch\n",
      "Epoch: 3/20...  Training Step: 280...  Training loss: 7.1952...  0.0490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 281...  Training loss: 7.1953...  0.0420 sec/batch\n",
      "Epoch: 3/20...  Training Step: 282...  Training loss: 7.1953...  0.0660 sec/batch\n",
      "Epoch: 3/20...  Training Step: 283...  Training loss: 7.1950...  0.0450 sec/batch\n",
      "Epoch: 3/20...  Training Step: 284...  Training loss: 7.1949...  0.0420 sec/batch\n",
      "Epoch: 3/20...  Training Step: 285...  Training loss: 7.1949...  0.0390 sec/batch\n",
      "Epoch: 3/20...  Training Step: 286...  Training loss: 7.1949...  0.0620 sec/batch\n",
      "Epoch: 3/20...  Training Step: 287...  Training loss: 7.1950...  0.0440 sec/batch\n",
      "Epoch: 3/20...  Training Step: 288...  Training loss: 7.1951...  0.0650 sec/batch\n",
      "Epoch: 3/20...  Training Step: 289...  Training loss: 7.1950...  0.0680 sec/batch\n",
      "Epoch: 3/20...  Training Step: 290...  Training loss: 7.1952...  0.0900 sec/batch\n",
      "Epoch: 3/20...  Training Step: 291...  Training loss: 7.1951...  0.0570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 292...  Training loss: 7.1952...  0.0600 sec/batch\n",
      "Epoch: 3/20...  Training Step: 293...  Training loss: 7.1953...  0.0640 sec/batch\n",
      "Epoch: 3/20...  Training Step: 294...  Training loss: 7.1948...  0.0740 sec/batch\n",
      "Epoch: 3/20...  Training Step: 295...  Training loss: 7.1952...  0.0580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 296...  Training loss: 7.1950...  0.0620 sec/batch\n",
      "Epoch: 3/20...  Training Step: 297...  Training loss: 7.1952...  0.0770 sec/batch\n",
      "Epoch: 3/20...  Training Step: 298...  Training loss: 7.1950...  0.0550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 299...  Training loss: 7.1952...  0.0600 sec/batch\n",
      "Epoch: 3/20...  Training Step: 300...  Training loss: 7.1956...  0.0570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 301...  Training loss: 7.1952...  0.0400 sec/batch\n",
      "Epoch: 3/20...  Training Step: 302...  Training loss: 7.1947...  0.0420 sec/batch\n",
      "Epoch: 3/20...  Training Step: 303...  Training loss: 7.1952...  0.0430 sec/batch\n",
      "Epoch: 3/20...  Training Step: 304...  Training loss: 7.1953...  0.0410 sec/batch\n",
      "Epoch: 3/20...  Training Step: 305...  Training loss: 7.1949...  0.0410 sec/batch\n",
      "Epoch: 3/20...  Training Step: 306...  Training loss: 7.1952...  0.0630 sec/batch\n",
      "Epoch: 3/20...  Training Step: 307...  Training loss: 7.1951...  0.0500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 308...  Training loss: 7.1947...  0.0630 sec/batch\n",
      "Epoch: 3/20...  Training Step: 309...  Training loss: 7.1954...  0.1020 sec/batch\n",
      "Epoch: 3/20...  Training Step: 310...  Training loss: 7.1948...  0.1160 sec/batch\n",
      "Epoch: 3/20...  Training Step: 311...  Training loss: 7.1950...  0.1080 sec/batch\n",
      "Epoch: 3/20...  Training Step: 312...  Training loss: 7.1949...  0.0520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 313...  Training loss: 7.1950...  0.0650 sec/batch\n",
      "Epoch: 3/20...  Training Step: 314...  Training loss: 7.1956...  0.0560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 315...  Training loss: 7.1949...  0.0630 sec/batch\n",
      "Epoch: 3/20...  Training Step: 316...  Training loss: 7.1953...  0.0750 sec/batch\n",
      "Epoch: 3/20...  Training Step: 317...  Training loss: 7.1952...  0.0530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 318...  Training loss: 7.1950...  0.0400 sec/batch\n",
      "Epoch: 3/20...  Training Step: 319...  Training loss: 7.1951...  0.0410 sec/batch\n",
      "Epoch: 3/20...  Training Step: 320...  Training loss: 7.1952...  0.0770 sec/batch\n",
      "Epoch: 3/20...  Training Step: 321...  Training loss: 7.1950...  0.0570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 322...  Training loss: 7.1950...  0.0490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 323...  Training loss: 7.1953...  0.0530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 324...  Training loss: 7.1951...  0.0530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 325...  Training loss: 7.1952...  0.0520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 326...  Training loss: 7.1949...  0.0750 sec/batch\n",
      "Epoch: 3/20...  Training Step: 327...  Training loss: 7.1955...  0.0910 sec/batch\n",
      "Epoch: 3/20...  Training Step: 328...  Training loss: 7.1949...  0.0520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 329...  Training loss: 7.1947...  0.0530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 330...  Training loss: 7.1946...  0.0670 sec/batch\n",
      "Epoch: 3/20...  Training Step: 331...  Training loss: 7.1951...  0.0450 sec/batch\n",
      "Epoch: 3/20...  Training Step: 332...  Training loss: 7.1949...  0.0450 sec/batch\n",
      "Epoch: 3/20...  Training Step: 333...  Training loss: 7.1948...  0.0710 sec/batch\n",
      "Epoch: 3/20...  Training Step: 334...  Training loss: 7.1951...  0.0520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 335...  Training loss: 7.1949...  0.0370 sec/batch\n",
      "Epoch: 3/20...  Training Step: 336...  Training loss: 7.1951...  0.0460 sec/batch\n",
      "Epoch: 3/20...  Training Step: 337...  Training loss: 7.1949...  0.0430 sec/batch\n",
      "Epoch: 3/20...  Training Step: 338...  Training loss: 7.1950...  0.0600 sec/batch\n",
      "Epoch: 3/20...  Training Step: 339...  Training loss: 7.1953...  0.0540 sec/batch\n",
      "Epoch: 3/20...  Training Step: 340...  Training loss: 7.1951...  0.0680 sec/batch\n",
      "Epoch: 3/20...  Training Step: 341...  Training loss: 7.1952...  0.0530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 342...  Training loss: 7.1952...  0.0530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 343...  Training loss: 7.1949...  0.0500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 344...  Training loss: 7.1949...  0.0490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 345...  Training loss: 7.1950...  0.0630 sec/batch\n",
      "Epoch: 3/20...  Training Step: 346...  Training loss: 7.1949...  0.0580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 347...  Training loss: 7.1949...  0.0830 sec/batch\n",
      "Epoch: 3/20...  Training Step: 348...  Training loss: 7.1951...  0.1050 sec/batch\n",
      "Epoch: 3/20...  Training Step: 349...  Training loss: 7.1949...  0.0820 sec/batch\n",
      "Epoch: 3/20...  Training Step: 350...  Training loss: 7.1949...  0.0590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 351...  Training loss: 7.1949...  0.0470 sec/batch\n",
      "Epoch: 4/20...  Training Step: 352...  Training loss: 7.1952...  0.0690 sec/batch\n",
      "Epoch: 4/20...  Training Step: 353...  Training loss: 7.1950...  0.0850 sec/batch\n",
      "Epoch: 4/20...  Training Step: 354...  Training loss: 7.1951...  0.0670 sec/batch\n",
      "Epoch: 4/20...  Training Step: 355...  Training loss: 7.1949...  0.0520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 356...  Training loss: 7.1950...  0.0520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 357...  Training loss: 7.1952...  0.0430 sec/batch\n",
      "Epoch: 4/20...  Training Step: 358...  Training loss: 7.1950...  0.0880 sec/batch\n",
      "Epoch: 4/20...  Training Step: 359...  Training loss: 7.1949...  0.1120 sec/batch\n",
      "Epoch: 4/20...  Training Step: 360...  Training loss: 7.1954...  0.0700 sec/batch\n",
      "Epoch: 4/20...  Training Step: 361...  Training loss: 7.1950...  0.0490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 362...  Training loss: 7.1956...  0.0530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 363...  Training loss: 7.1950...  0.0540 sec/batch\n",
      "Epoch: 4/20...  Training Step: 364...  Training loss: 7.1950...  0.0510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 365...  Training loss: 7.1950...  0.0740 sec/batch\n",
      "Epoch: 4/20...  Training Step: 366...  Training loss: 7.1949...  0.0710 sec/batch\n",
      "Epoch: 4/20...  Training Step: 367...  Training loss: 7.1952...  0.0570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 368...  Training loss: 7.1950...  0.0860 sec/batch\n",
      "Epoch: 4/20...  Training Step: 369...  Training loss: 7.1955...  0.0600 sec/batch\n",
      "Epoch: 4/20...  Training Step: 370...  Training loss: 7.1949...  0.0670 sec/batch\n",
      "Epoch: 4/20...  Training Step: 371...  Training loss: 7.1949...  0.1050 sec/batch\n",
      "Epoch: 4/20...  Training Step: 372...  Training loss: 7.1950...  0.0480 sec/batch\n",
      "Epoch: 4/20...  Training Step: 373...  Training loss: 7.1949...  0.0430 sec/batch\n",
      "Epoch: 4/20...  Training Step: 374...  Training loss: 7.1951...  0.0700 sec/batch\n",
      "Epoch: 4/20...  Training Step: 375...  Training loss: 7.1949...  0.0540 sec/batch\n",
      "Epoch: 4/20...  Training Step: 376...  Training loss: 7.1950...  0.0560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 377...  Training loss: 7.1951...  0.0600 sec/batch\n",
      "Epoch: 4/20...  Training Step: 378...  Training loss: 7.1949...  0.0700 sec/batch\n",
      "Epoch: 4/20...  Training Step: 379...  Training loss: 7.1949...  0.0610 sec/batch\n",
      "Epoch: 4/20...  Training Step: 380...  Training loss: 7.1949...  0.0780 sec/batch\n",
      "Epoch: 4/20...  Training Step: 381...  Training loss: 7.1949...  0.0480 sec/batch\n",
      "Epoch: 4/20...  Training Step: 382...  Training loss: 7.1951...  0.0460 sec/batch\n",
      "Epoch: 4/20...  Training Step: 383...  Training loss: 7.1949...  0.0530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 384...  Training loss: 7.1953...  0.0470 sec/batch\n",
      "Epoch: 4/20...  Training Step: 385...  Training loss: 7.1945...  0.0480 sec/batch\n",
      "Epoch: 4/20...  Training Step: 386...  Training loss: 7.1952...  0.0700 sec/batch\n",
      "Epoch: 4/20...  Training Step: 387...  Training loss: 7.1951...  0.0530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 388...  Training loss: 7.1952...  0.0790 sec/batch\n",
      "Epoch: 4/20...  Training Step: 389...  Training loss: 7.1947...  0.0460 sec/batch\n",
      "Epoch: 4/20...  Training Step: 390...  Training loss: 7.1951...  0.0660 sec/batch\n",
      "Epoch: 4/20...  Training Step: 391...  Training loss: 7.1947...  0.0430 sec/batch\n",
      "Epoch: 4/20...  Training Step: 392...  Training loss: 7.1948...  0.0700 sec/batch\n",
      "Epoch: 4/20...  Training Step: 393...  Training loss: 7.1947...  0.0470 sec/batch\n",
      "Epoch: 4/20...  Training Step: 394...  Training loss: 7.1952...  0.0420 sec/batch\n",
      "Epoch: 4/20...  Training Step: 395...  Training loss: 7.1947...  0.0620 sec/batch\n",
      "Epoch: 4/20...  Training Step: 396...  Training loss: 7.1948...  0.0770 sec/batch\n",
      "Epoch: 4/20...  Training Step: 397...  Training loss: 7.1950...  0.0900 sec/batch\n",
      "Epoch: 4/20...  Training Step: 398...  Training loss: 7.1949...  0.0620 sec/batch\n",
      "Epoch: 4/20...  Training Step: 399...  Training loss: 7.1951...  0.0430 sec/batch\n",
      "Epoch: 4/20...  Training Step: 400...  Training loss: 7.1947...  0.0500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 401...  Training loss: 7.1952...  0.0420 sec/batch\n",
      "Epoch: 4/20...  Training Step: 402...  Training loss: 7.1945...  0.0410 sec/batch\n",
      "Epoch: 4/20...  Training Step: 403...  Training loss: 7.1948...  0.0410 sec/batch\n",
      "Epoch: 4/20...  Training Step: 404...  Training loss: 7.1950...  0.0440 sec/batch\n",
      "Epoch: 4/20...  Training Step: 405...  Training loss: 7.1946...  0.0420 sec/batch\n",
      "Epoch: 4/20...  Training Step: 406...  Training loss: 7.1946...  0.0390 sec/batch\n",
      "Epoch: 4/20...  Training Step: 407...  Training loss: 7.1948...  0.0710 sec/batch\n",
      "Epoch: 4/20...  Training Step: 408...  Training loss: 7.1948...  0.0540 sec/batch\n",
      "Epoch: 4/20...  Training Step: 409...  Training loss: 7.1947...  0.0590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 410...  Training loss: 7.1951...  0.0400 sec/batch\n",
      "Epoch: 4/20...  Training Step: 411...  Training loss: 7.1949...  0.0460 sec/batch\n",
      "Epoch: 4/20...  Training Step: 412...  Training loss: 7.1947...  0.0470 sec/batch\n",
      "Epoch: 4/20...  Training Step: 413...  Training loss: 7.1947...  0.0590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 414...  Training loss: 7.1948...  0.0600 sec/batch\n",
      "Epoch: 4/20...  Training Step: 415...  Training loss: 7.1952...  0.0420 sec/batch\n",
      "Epoch: 4/20...  Training Step: 416...  Training loss: 7.1948...  0.0570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 417...  Training loss: 7.1953...  0.0560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 418...  Training loss: 7.1950...  0.0570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 419...  Training loss: 7.1946...  0.0420 sec/batch\n",
      "Epoch: 4/20...  Training Step: 420...  Training loss: 7.1948...  0.0600 sec/batch\n",
      "Epoch: 4/20...  Training Step: 421...  Training loss: 7.1954...  0.0530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 422...  Training loss: 7.1944...  0.0610 sec/batch\n",
      "Epoch: 4/20...  Training Step: 423...  Training loss: 7.1948...  0.0940 sec/batch\n",
      "Epoch: 4/20...  Training Step: 424...  Training loss: 7.1947...  0.0760 sec/batch\n",
      "Epoch: 4/20...  Training Step: 425...  Training loss: 7.1951...  0.0680 sec/batch\n",
      "Epoch: 4/20...  Training Step: 426...  Training loss: 7.1952...  0.0480 sec/batch\n",
      "Epoch: 4/20...  Training Step: 427...  Training loss: 7.1950...  0.0700 sec/batch\n",
      "Epoch: 4/20...  Training Step: 428...  Training loss: 7.1945...  0.0630 sec/batch\n",
      "Epoch: 4/20...  Training Step: 429...  Training loss: 7.1950...  0.0450 sec/batch\n",
      "Epoch: 4/20...  Training Step: 430...  Training loss: 7.1943...  0.0580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 431...  Training loss: 7.1950...  0.0720 sec/batch\n",
      "Epoch: 4/20...  Training Step: 432...  Training loss: 7.1947...  0.0830 sec/batch\n",
      "Epoch: 4/20...  Training Step: 433...  Training loss: 7.1947...  0.0570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 434...  Training loss: 7.1950...  0.0470 sec/batch\n",
      "Epoch: 4/20...  Training Step: 435...  Training loss: 7.1946...  0.0690 sec/batch\n",
      "Epoch: 4/20...  Training Step: 436...  Training loss: 7.1948...  0.0920 sec/batch\n",
      "Epoch: 4/20...  Training Step: 437...  Training loss: 7.1950...  0.0550 sec/batch\n",
      "Epoch: 4/20...  Training Step: 438...  Training loss: 7.1951...  0.0460 sec/batch\n",
      "Epoch: 4/20...  Training Step: 439...  Training loss: 7.1949...  0.0530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 440...  Training loss: 7.1949...  0.0490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 441...  Training loss: 7.1948...  0.0390 sec/batch\n",
      "Epoch: 4/20...  Training Step: 442...  Training loss: 7.1949...  0.0390 sec/batch\n",
      "Epoch: 4/20...  Training Step: 443...  Training loss: 7.1947...  0.0370 sec/batch\n",
      "Epoch: 4/20...  Training Step: 444...  Training loss: 7.1949...  0.0410 sec/batch\n",
      "Epoch: 4/20...  Training Step: 445...  Training loss: 7.1948...  0.0400 sec/batch\n",
      "Epoch: 4/20...  Training Step: 446...  Training loss: 7.1948...  0.0420 sec/batch\n",
      "Epoch: 4/20...  Training Step: 447...  Training loss: 7.1945...  0.0640 sec/batch\n",
      "Epoch: 4/20...  Training Step: 448...  Training loss: 7.1947...  0.0490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 449...  Training loss: 7.1949...  0.0580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 450...  Training loss: 7.1944...  0.0410 sec/batch\n",
      "Epoch: 4/20...  Training Step: 451...  Training loss: 7.1946...  0.0450 sec/batch\n",
      "Epoch: 4/20...  Training Step: 452...  Training loss: 7.1949...  0.0610 sec/batch\n",
      "Epoch: 4/20...  Training Step: 453...  Training loss: 7.1950...  0.0420 sec/batch\n",
      "Epoch: 4/20...  Training Step: 454...  Training loss: 7.1946...  0.0690 sec/batch\n",
      "Epoch: 4/20...  Training Step: 455...  Training loss: 7.1944...  0.0510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 456...  Training loss: 7.1950...  0.0430 sec/batch\n",
      "Epoch: 4/20...  Training Step: 457...  Training loss: 7.1946...  0.0530 sec/batch\n",
      "Epoch: 4/20...  Training Step: 458...  Training loss: 7.1948...  0.0770 sec/batch\n",
      "Epoch: 4/20...  Training Step: 459...  Training loss: 7.1950...  0.0650 sec/batch\n",
      "Epoch: 4/20...  Training Step: 460...  Training loss: 7.1948...  0.0830 sec/batch\n",
      "Epoch: 4/20...  Training Step: 461...  Training loss: 7.1943...  0.0460 sec/batch\n",
      "Epoch: 4/20...  Training Step: 462...  Training loss: 7.1950...  0.0510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 463...  Training loss: 7.1945...  0.0440 sec/batch\n",
      "Epoch: 4/20...  Training Step: 464...  Training loss: 7.1946...  0.0380 sec/batch\n",
      "Epoch: 4/20...  Training Step: 465...  Training loss: 7.1942...  0.0370 sec/batch\n",
      "Epoch: 4/20...  Training Step: 466...  Training loss: 7.1947...  0.0450 sec/batch\n",
      "Epoch: 4/20...  Training Step: 467...  Training loss: 7.1949...  0.0480 sec/batch\n",
      "Epoch: 4/20...  Training Step: 468...  Training loss: 7.1947...  0.0800 sec/batch\n",
      "Epoch: 5/20...  Training Step: 469...  Training loss: 7.1949...  0.0900 sec/batch\n",
      "Epoch: 5/20...  Training Step: 470...  Training loss: 7.1944...  0.0660 sec/batch\n",
      "Epoch: 5/20...  Training Step: 471...  Training loss: 7.1948...  0.0590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 472...  Training loss: 7.1947...  0.0610 sec/batch\n",
      "Epoch: 5/20...  Training Step: 473...  Training loss: 7.1947...  0.0490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 474...  Training loss: 7.1947...  0.0620 sec/batch\n",
      "Epoch: 5/20...  Training Step: 475...  Training loss: 7.1949...  0.0740 sec/batch\n",
      "Epoch: 5/20...  Training Step: 476...  Training loss: 7.1945...  0.0420 sec/batch\n",
      "Epoch: 5/20...  Training Step: 477...  Training loss: 7.1953...  0.0710 sec/batch\n",
      "Epoch: 5/20...  Training Step: 478...  Training loss: 7.1946...  0.0430 sec/batch\n",
      "Epoch: 5/20...  Training Step: 479...  Training loss: 7.1952...  0.0830 sec/batch\n",
      "Epoch: 5/20...  Training Step: 480...  Training loss: 7.1951...  0.0530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 481...  Training loss: 7.1950...  0.0700 sec/batch\n",
      "Epoch: 5/20...  Training Step: 482...  Training loss: 7.1946...  0.0630 sec/batch\n",
      "Epoch: 5/20...  Training Step: 483...  Training loss: 7.1946...  0.0690 sec/batch\n",
      "Epoch: 5/20...  Training Step: 484...  Training loss: 7.1947...  0.0510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 485...  Training loss: 7.1950...  0.0580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 486...  Training loss: 7.1950...  0.0490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 487...  Training loss: 7.1948...  0.0660 sec/batch\n",
      "Epoch: 5/20...  Training Step: 488...  Training loss: 7.1947...  0.0640 sec/batch\n",
      "Epoch: 5/20...  Training Step: 489...  Training loss: 7.1945...  0.0550 sec/batch\n",
      "Epoch: 5/20...  Training Step: 490...  Training loss: 7.1948...  0.0520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 491...  Training loss: 7.1950...  0.0430 sec/batch\n",
      "Epoch: 5/20...  Training Step: 492...  Training loss: 7.1949...  0.0590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 493...  Training loss: 7.1946...  0.0590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 494...  Training loss: 7.1945...  0.0680 sec/batch\n",
      "Epoch: 5/20...  Training Step: 495...  Training loss: 7.1939...  0.0630 sec/batch\n",
      "Epoch: 5/20...  Training Step: 496...  Training loss: 7.1946...  0.0710 sec/batch\n",
      "Epoch: 5/20...  Training Step: 497...  Training loss: 7.1949...  0.0770 sec/batch\n",
      "Epoch: 5/20...  Training Step: 498...  Training loss: 7.1944...  0.0660 sec/batch\n",
      "Epoch: 5/20...  Training Step: 499...  Training loss: 7.1945...  0.0660 sec/batch\n",
      "Epoch: 5/20...  Training Step: 500...  Training loss: 7.1948...  0.0640 sec/batch\n",
      "Epoch: 5/20...  Training Step: 501...  Training loss: 7.1947...  0.0400 sec/batch\n",
      "Epoch: 5/20...  Training Step: 502...  Training loss: 7.1947...  0.0400 sec/batch\n",
      "Epoch: 5/20...  Training Step: 503...  Training loss: 7.1947...  0.0360 sec/batch\n",
      "Epoch: 5/20...  Training Step: 504...  Training loss: 7.1946...  0.0370 sec/batch\n",
      "Epoch: 5/20...  Training Step: 505...  Training loss: 7.1947...  0.0380 sec/batch\n",
      "Epoch: 5/20...  Training Step: 506...  Training loss: 7.1948...  0.0420 sec/batch\n",
      "Epoch: 5/20...  Training Step: 507...  Training loss: 7.1942...  0.0420 sec/batch\n",
      "Epoch: 5/20...  Training Step: 508...  Training loss: 7.1946...  0.0430 sec/batch\n",
      "Epoch: 5/20...  Training Step: 509...  Training loss: 7.1943...  0.0400 sec/batch\n",
      "Epoch: 5/20...  Training Step: 510...  Training loss: 7.1944...  0.0450 sec/batch\n",
      "Epoch: 5/20...  Training Step: 511...  Training loss: 7.1945...  0.0450 sec/batch\n",
      "Epoch: 5/20...  Training Step: 512...  Training loss: 7.1948...  0.0440 sec/batch\n",
      "Epoch: 5/20...  Training Step: 513...  Training loss: 7.1942...  0.0560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 514...  Training loss: 7.1942...  0.0510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 515...  Training loss: 7.1948...  0.0390 sec/batch\n",
      "Epoch: 5/20...  Training Step: 516...  Training loss: 7.1949...  0.0410 sec/batch\n",
      "Epoch: 5/20...  Training Step: 517...  Training loss: 7.1944...  0.0520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 518...  Training loss: 7.1945...  0.0470 sec/batch\n",
      "Epoch: 5/20...  Training Step: 519...  Training loss: 7.1945...  0.0450 sec/batch\n",
      "Epoch: 5/20...  Training Step: 520...  Training loss: 7.1946...  0.0360 sec/batch\n",
      "Epoch: 5/20...  Training Step: 521...  Training loss: 7.1943...  0.0460 sec/batch\n",
      "Epoch: 5/20...  Training Step: 522...  Training loss: 7.1944...  0.0370 sec/batch\n",
      "Epoch: 5/20...  Training Step: 523...  Training loss: 7.1947...  0.0430 sec/batch\n",
      "Epoch: 5/20...  Training Step: 524...  Training loss: 7.1949...  0.0390 sec/batch\n",
      "Epoch: 5/20...  Training Step: 525...  Training loss: 7.1946...  0.0380 sec/batch\n",
      "Epoch: 5/20...  Training Step: 526...  Training loss: 7.1945...  0.0370 sec/batch\n",
      "Epoch: 5/20...  Training Step: 527...  Training loss: 7.1948...  0.0580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 528...  Training loss: 7.1946...  0.0500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 529...  Training loss: 7.1943...  0.0400 sec/batch\n",
      "Epoch: 5/20...  Training Step: 530...  Training loss: 7.1947...  0.0420 sec/batch\n",
      "Epoch: 5/20...  Training Step: 531...  Training loss: 7.1942...  0.0360 sec/batch\n",
      "Epoch: 5/20...  Training Step: 532...  Training loss: 7.1948...  0.0410 sec/batch\n",
      "Epoch: 5/20...  Training Step: 533...  Training loss: 7.1946...  0.0480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 534...  Training loss: 7.1949...  0.0380 sec/batch\n",
      "Epoch: 5/20...  Training Step: 535...  Training loss: 7.1947...  0.0560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 536...  Training loss: 7.1946...  0.0390 sec/batch\n",
      "Epoch: 5/20...  Training Step: 537...  Training loss: 7.1948...  0.0580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 538...  Training loss: 7.1947...  0.0480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 539...  Training loss: 7.1940...  0.0420 sec/batch\n",
      "Epoch: 5/20...  Training Step: 540...  Training loss: 7.1946...  0.0420 sec/batch\n",
      "Epoch: 5/20...  Training Step: 541...  Training loss: 7.1944...  0.0420 sec/batch\n",
      "Epoch: 5/20...  Training Step: 542...  Training loss: 7.1940...  0.0410 sec/batch\n",
      "Epoch: 5/20...  Training Step: 543...  Training loss: 7.1946...  0.0460 sec/batch\n",
      "Epoch: 5/20...  Training Step: 544...  Training loss: 7.1944...  0.0410 sec/batch\n",
      "Epoch: 5/20...  Training Step: 545...  Training loss: 7.1945...  0.0410 sec/batch\n",
      "Epoch: 5/20...  Training Step: 546...  Training loss: 7.1948...  0.0770 sec/batch\n",
      "Epoch: 5/20...  Training Step: 547...  Training loss: 7.1940...  0.0800 sec/batch\n",
      "Epoch: 5/20...  Training Step: 548...  Training loss: 7.1944...  0.0550 sec/batch\n",
      "Epoch: 5/20...  Training Step: 549...  Training loss: 7.1944...  0.0950 sec/batch\n",
      "Epoch: 5/20...  Training Step: 550...  Training loss: 7.1945...  0.0640 sec/batch\n",
      "Epoch: 5/20...  Training Step: 551...  Training loss: 7.1943...  0.0480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 552...  Training loss: 7.1944...  0.0490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 553...  Training loss: 7.1944...  0.0600 sec/batch\n",
      "Epoch: 5/20...  Training Step: 554...  Training loss: 7.1945...  0.0640 sec/batch\n",
      "Epoch: 5/20...  Training Step: 555...  Training loss: 7.1944...  0.0470 sec/batch\n",
      "Epoch: 5/20...  Training Step: 556...  Training loss: 7.1946...  0.0540 sec/batch\n",
      "Epoch: 5/20...  Training Step: 557...  Training loss: 7.1945...  0.0530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 558...  Training loss: 7.1944...  0.0640 sec/batch\n",
      "Epoch: 5/20...  Training Step: 559...  Training loss: 7.1948...  0.0410 sec/batch\n",
      "Epoch: 5/20...  Training Step: 560...  Training loss: 7.1944...  0.0440 sec/batch\n",
      "Epoch: 5/20...  Training Step: 561...  Training loss: 7.1946...  0.0410 sec/batch\n",
      "Epoch: 5/20...  Training Step: 562...  Training loss: 7.1944...  0.0450 sec/batch\n",
      "Epoch: 5/20...  Training Step: 563...  Training loss: 7.1940...  0.0540 sec/batch\n",
      "Epoch: 5/20...  Training Step: 564...  Training loss: 7.1943...  0.0400 sec/batch\n",
      "Epoch: 5/20...  Training Step: 565...  Training loss: 7.1943...  0.0430 sec/batch\n",
      "Epoch: 5/20...  Training Step: 566...  Training loss: 7.1944...  0.0460 sec/batch\n",
      "Epoch: 5/20...  Training Step: 567...  Training loss: 7.1941...  0.0550 sec/batch\n",
      "Epoch: 5/20...  Training Step: 568...  Training loss: 7.1941...  0.0550 sec/batch\n",
      "Epoch: 5/20...  Training Step: 569...  Training loss: 7.1946...  0.0760 sec/batch\n",
      "Epoch: 5/20...  Training Step: 570...  Training loss: 7.1949...  0.0890 sec/batch\n",
      "Epoch: 5/20...  Training Step: 571...  Training loss: 7.1939...  0.0730 sec/batch\n",
      "Epoch: 5/20...  Training Step: 572...  Training loss: 7.1943...  0.0490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 573...  Training loss: 7.1948...  0.0490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 574...  Training loss: 7.1943...  0.0570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 575...  Training loss: 7.1951...  0.0480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 576...  Training loss: 7.1942...  0.0550 sec/batch\n",
      "Epoch: 5/20...  Training Step: 577...  Training loss: 7.1945...  0.0490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 578...  Training loss: 7.1942...  0.0800 sec/batch\n",
      "Epoch: 5/20...  Training Step: 579...  Training loss: 7.1950...  0.0480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 580...  Training loss: 7.1945...  0.0390 sec/batch\n",
      "Epoch: 5/20...  Training Step: 581...  Training loss: 7.1939...  0.0460 sec/batch\n",
      "Epoch: 5/20...  Training Step: 582...  Training loss: 7.1947...  0.0570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 583...  Training loss: 7.1943...  0.0720 sec/batch\n",
      "Epoch: 5/20...  Training Step: 584...  Training loss: 7.1946...  0.0700 sec/batch\n",
      "Epoch: 5/20...  Training Step: 585...  Training loss: 7.1945...  0.0650 sec/batch\n",
      "Epoch: 6/20...  Training Step: 586...  Training loss: 7.1948...  0.0470 sec/batch\n",
      "Epoch: 6/20...  Training Step: 587...  Training loss: 7.1943...  0.0450 sec/batch\n",
      "Epoch: 6/20...  Training Step: 588...  Training loss: 7.1951...  0.0530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 589...  Training loss: 7.1942...  0.0770 sec/batch\n",
      "Epoch: 6/20...  Training Step: 590...  Training loss: 7.1946...  0.0590 sec/batch\n",
      "Epoch: 6/20...  Training Step: 591...  Training loss: 7.1945...  0.0450 sec/batch\n",
      "Epoch: 6/20...  Training Step: 592...  Training loss: 7.1945...  0.0450 sec/batch\n",
      "Epoch: 6/20...  Training Step: 593...  Training loss: 7.1944...  0.0560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 594...  Training loss: 7.1952...  0.0740 sec/batch\n",
      "Epoch: 6/20...  Training Step: 595...  Training loss: 7.1944...  0.0560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 596...  Training loss: 7.1946...  0.0510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 597...  Training loss: 7.1948...  0.0600 sec/batch\n",
      "Epoch: 6/20...  Training Step: 598...  Training loss: 7.1947...  0.0600 sec/batch\n",
      "Epoch: 6/20...  Training Step: 599...  Training loss: 7.1944...  0.0620 sec/batch\n",
      "Epoch: 6/20...  Training Step: 600...  Training loss: 7.1941...  0.0520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 601...  Training loss: 7.1947...  0.0410 sec/batch\n",
      "Epoch: 6/20...  Training Step: 602...  Training loss: 7.1943...  0.0400 sec/batch\n",
      "Epoch: 6/20...  Training Step: 603...  Training loss: 7.1942...  0.0440 sec/batch\n",
      "Epoch: 6/20...  Training Step: 604...  Training loss: 7.1938...  0.0400 sec/batch\n",
      "Epoch: 6/20...  Training Step: 605...  Training loss: 7.1944...  0.0390 sec/batch\n",
      "Epoch: 6/20...  Training Step: 606...  Training loss: 7.1946...  0.0540 sec/batch\n",
      "Epoch: 6/20...  Training Step: 607...  Training loss: 7.1942...  0.0630 sec/batch\n",
      "Epoch: 6/20...  Training Step: 608...  Training loss: 7.1948...  0.0850 sec/batch\n",
      "Epoch: 6/20...  Training Step: 609...  Training loss: 7.1937...  0.0510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 610...  Training loss: 7.1937...  0.0480 sec/batch\n",
      "Epoch: 6/20...  Training Step: 611...  Training loss: 7.1952...  0.0530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 612...  Training loss: 7.1942...  0.0610 sec/batch\n",
      "Epoch: 6/20...  Training Step: 613...  Training loss: 7.1939...  0.0410 sec/batch\n",
      "Epoch: 6/20...  Training Step: 614...  Training loss: 7.1942...  0.0730 sec/batch\n",
      "Epoch: 6/20...  Training Step: 615...  Training loss: 7.1937...  0.0560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 616...  Training loss: 7.1943...  0.0500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 617...  Training loss: 7.1943...  0.0420 sec/batch\n",
      "Epoch: 6/20...  Training Step: 618...  Training loss: 7.1948...  0.0440 sec/batch\n",
      "Epoch: 6/20...  Training Step: 619...  Training loss: 7.1941...  0.0510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 620...  Training loss: 7.1944...  0.0490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 621...  Training loss: 7.1942...  0.0390 sec/batch\n",
      "Epoch: 6/20...  Training Step: 622...  Training loss: 7.1946...  0.0550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 623...  Training loss: 7.1941...  0.0480 sec/batch\n",
      "Epoch: 6/20...  Training Step: 624...  Training loss: 7.1945...  0.0450 sec/batch\n",
      "Epoch: 6/20...  Training Step: 625...  Training loss: 7.1942...  0.0510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 626...  Training loss: 7.1940...  0.0600 sec/batch\n",
      "Epoch: 6/20...  Training Step: 627...  Training loss: 7.1948...  0.0600 sec/batch\n",
      "Epoch: 6/20...  Training Step: 628...  Training loss: 7.1943...  0.0770 sec/batch\n",
      "Epoch: 6/20...  Training Step: 629...  Training loss: 7.1943...  0.0440 sec/batch\n",
      "Epoch: 6/20...  Training Step: 630...  Training loss: 7.1943...  0.0470 sec/batch\n",
      "Epoch: 6/20...  Training Step: 631...  Training loss: 7.1943...  0.0610 sec/batch\n",
      "Epoch: 6/20...  Training Step: 632...  Training loss: 7.1945...  0.0440 sec/batch\n",
      "Epoch: 6/20...  Training Step: 633...  Training loss: 7.1944...  0.0480 sec/batch\n",
      "Epoch: 6/20...  Training Step: 634...  Training loss: 7.1945...  0.0510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 635...  Training loss: 7.1950...  0.0680 sec/batch\n",
      "Epoch: 6/20...  Training Step: 636...  Training loss: 7.1937...  0.0590 sec/batch\n",
      "Epoch: 6/20...  Training Step: 637...  Training loss: 7.1942...  0.0720 sec/batch\n",
      "Epoch: 6/20...  Training Step: 638...  Training loss: 7.1940...  0.0840 sec/batch\n",
      "Epoch: 6/20...  Training Step: 639...  Training loss: 7.1941...  0.0410 sec/batch\n",
      "Epoch: 6/20...  Training Step: 640...  Training loss: 7.1940...  0.0520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 641...  Training loss: 7.1942...  0.0540 sec/batch\n",
      "Epoch: 6/20...  Training Step: 642...  Training loss: 7.1940...  0.0540 sec/batch\n",
      "Epoch: 6/20...  Training Step: 643...  Training loss: 7.1944...  0.0580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 644...  Training loss: 7.1947...  0.0620 sec/batch\n",
      "Epoch: 6/20...  Training Step: 645...  Training loss: 7.1938...  0.0570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 646...  Training loss: 7.1941...  0.0460 sec/batch\n",
      "Epoch: 6/20...  Training Step: 647...  Training loss: 7.1949...  0.0480 sec/batch\n",
      "Epoch: 6/20...  Training Step: 648...  Training loss: 7.1938...  0.0560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 649...  Training loss: 7.1939...  0.0720 sec/batch\n",
      "Epoch: 6/20...  Training Step: 650...  Training loss: 7.1941...  0.0800 sec/batch\n",
      "Epoch: 6/20...  Training Step: 651...  Training loss: 7.1946...  0.0590 sec/batch\n",
      "Epoch: 6/20...  Training Step: 652...  Training loss: 7.1945...  0.0640 sec/batch\n",
      "Epoch: 6/20...  Training Step: 653...  Training loss: 7.1945...  0.0700 sec/batch\n",
      "Epoch: 6/20...  Training Step: 654...  Training loss: 7.1942...  0.0480 sec/batch\n",
      "Epoch: 6/20...  Training Step: 655...  Training loss: 7.1947...  0.0540 sec/batch\n",
      "Epoch: 6/20...  Training Step: 656...  Training loss: 7.1939...  0.0760 sec/batch\n",
      "Epoch: 6/20...  Training Step: 657...  Training loss: 7.1945...  0.0550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 658...  Training loss: 7.1940...  0.0490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 659...  Training loss: 7.1942...  0.0500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 660...  Training loss: 7.1946...  0.0480 sec/batch\n",
      "Epoch: 6/20...  Training Step: 661...  Training loss: 7.1941...  0.0420 sec/batch\n",
      "Epoch: 6/20...  Training Step: 662...  Training loss: 7.1939...  0.0440 sec/batch\n",
      "Epoch: 6/20...  Training Step: 663...  Training loss: 7.1941...  0.0400 sec/batch\n",
      "Epoch: 6/20...  Training Step: 664...  Training loss: 7.1938...  0.0430 sec/batch\n",
      "Epoch: 6/20...  Training Step: 665...  Training loss: 7.1942...  0.0480 sec/batch\n",
      "Epoch: 6/20...  Training Step: 666...  Training loss: 7.1938...  0.0480 sec/batch\n",
      "Epoch: 6/20...  Training Step: 667...  Training loss: 7.1942...  0.0460 sec/batch\n",
      "Epoch: 6/20...  Training Step: 668...  Training loss: 7.1940...  0.0620 sec/batch\n",
      "Epoch: 6/20...  Training Step: 669...  Training loss: 7.1941...  0.0600 sec/batch\n",
      "Epoch: 6/20...  Training Step: 670...  Training loss: 7.1940...  0.0460 sec/batch\n",
      "Epoch: 6/20...  Training Step: 671...  Training loss: 7.1944...  0.0530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 672...  Training loss: 7.1941...  0.0520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 673...  Training loss: 7.1938...  0.0710 sec/batch\n",
      "Epoch: 6/20...  Training Step: 674...  Training loss: 7.1944...  0.0640 sec/batch\n",
      "Epoch: 6/20...  Training Step: 675...  Training loss: 7.1938...  0.0580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 676...  Training loss: 7.1940...  0.0430 sec/batch\n",
      "Epoch: 6/20...  Training Step: 677...  Training loss: 7.1941...  0.0650 sec/batch\n",
      "Epoch: 6/20...  Training Step: 678...  Training loss: 7.1946...  0.0520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 679...  Training loss: 7.1941...  0.0470 sec/batch\n",
      "Epoch: 6/20...  Training Step: 680...  Training loss: 7.1938...  0.0600 sec/batch\n",
      "Epoch: 6/20...  Training Step: 681...  Training loss: 7.1938...  0.0750 sec/batch\n",
      "Epoch: 6/20...  Training Step: 682...  Training loss: 7.1941...  0.0570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 683...  Training loss: 7.1943...  0.0690 sec/batch\n",
      "Epoch: 6/20...  Training Step: 684...  Training loss: 7.1941...  0.0670 sec/batch\n",
      "Epoch: 6/20...  Training Step: 685...  Training loss: 7.1944...  0.0770 sec/batch\n",
      "Epoch: 6/20...  Training Step: 686...  Training loss: 7.1939...  0.0490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 687...  Training loss: 7.1944...  0.0610 sec/batch\n",
      "Epoch: 6/20...  Training Step: 688...  Training loss: 7.1940...  0.0450 sec/batch\n",
      "Epoch: 6/20...  Training Step: 689...  Training loss: 7.1942...  0.0400 sec/batch\n",
      "Epoch: 6/20...  Training Step: 690...  Training loss: 7.1944...  0.0550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 691...  Training loss: 7.1938...  0.0540 sec/batch\n",
      "Epoch: 6/20...  Training Step: 692...  Training loss: 7.1945...  0.0740 sec/batch\n",
      "Epoch: 6/20...  Training Step: 693...  Training loss: 7.1940...  0.0460 sec/batch\n",
      "Epoch: 6/20...  Training Step: 694...  Training loss: 7.1949...  0.0470 sec/batch\n",
      "Epoch: 6/20...  Training Step: 695...  Training loss: 7.1936...  0.0460 sec/batch\n",
      "Epoch: 6/20...  Training Step: 696...  Training loss: 7.1943...  0.0540 sec/batch\n",
      "Epoch: 6/20...  Training Step: 697...  Training loss: 7.1937...  0.0450 sec/batch\n",
      "Epoch: 6/20...  Training Step: 698...  Training loss: 7.1942...  0.0400 sec/batch\n",
      "Epoch: 6/20...  Training Step: 699...  Training loss: 7.1942...  0.0630 sec/batch\n",
      "Epoch: 6/20...  Training Step: 700...  Training loss: 7.1937...  0.0510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 701...  Training loss: 7.1940...  0.0470 sec/batch\n",
      "Epoch: 6/20...  Training Step: 702...  Training loss: 7.1942...  0.0480 sec/batch\n",
      "Epoch: 7/20...  Training Step: 703...  Training loss: 7.1945...  0.0550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 704...  Training loss: 7.1940...  0.0420 sec/batch\n",
      "Epoch: 7/20...  Training Step: 705...  Training loss: 7.1947...  0.0420 sec/batch\n",
      "Epoch: 7/20...  Training Step: 706...  Training loss: 7.1939...  0.0660 sec/batch\n",
      "Epoch: 7/20...  Training Step: 707...  Training loss: 7.1939...  0.0560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 708...  Training loss: 7.1945...  0.0550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 709...  Training loss: 7.1940...  0.0480 sec/batch\n",
      "Epoch: 7/20...  Training Step: 710...  Training loss: 7.1941...  0.0540 sec/batch\n",
      "Epoch: 7/20...  Training Step: 711...  Training loss: 7.1951...  0.0760 sec/batch\n",
      "Epoch: 7/20...  Training Step: 712...  Training loss: 7.1943...  0.0500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 713...  Training loss: 7.1951...  0.0620 sec/batch\n",
      "Epoch: 7/20...  Training Step: 714...  Training loss: 7.1948...  0.0560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 715...  Training loss: 7.1940...  0.0590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 716...  Training loss: 7.1942...  0.0880 sec/batch\n",
      "Epoch: 7/20...  Training Step: 717...  Training loss: 7.1944...  0.0960 sec/batch\n",
      "Epoch: 7/20...  Training Step: 718...  Training loss: 7.1943...  0.0810 sec/batch\n",
      "Epoch: 7/20...  Training Step: 719...  Training loss: 7.1944...  0.0600 sec/batch\n",
      "Epoch: 7/20...  Training Step: 720...  Training loss: 7.1941...  0.0700 sec/batch\n",
      "Epoch: 7/20...  Training Step: 721...  Training loss: 7.1940...  0.0470 sec/batch\n",
      "Epoch: 7/20...  Training Step: 722...  Training loss: 7.1940...  0.0440 sec/batch\n",
      "Epoch: 7/20...  Training Step: 723...  Training loss: 7.1943...  0.0450 sec/batch\n",
      "Epoch: 7/20...  Training Step: 724...  Training loss: 7.1944...  0.0480 sec/batch\n",
      "Epoch: 7/20...  Training Step: 725...  Training loss: 7.1945...  0.0440 sec/batch\n",
      "Epoch: 7/20...  Training Step: 726...  Training loss: 7.1929...  0.0600 sec/batch\n",
      "Epoch: 7/20...  Training Step: 727...  Training loss: 7.1935...  0.0560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 728...  Training loss: 7.1943...  0.0490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 729...  Training loss: 7.1942...  0.0520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 730...  Training loss: 7.1942...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 731...  Training loss: 7.1940...  0.0660 sec/batch\n",
      "Epoch: 7/20...  Training Step: 732...  Training loss: 7.1933...  0.0520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 733...  Training loss: 7.1942...  0.0740 sec/batch\n",
      "Epoch: 7/20...  Training Step: 734...  Training loss: 7.1939...  0.0640 sec/batch\n",
      "Epoch: 7/20...  Training Step: 735...  Training loss: 7.1944...  0.0570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 736...  Training loss: 7.1937...  0.0480 sec/batch\n",
      "Epoch: 7/20...  Training Step: 737...  Training loss: 7.1945...  0.0560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 738...  Training loss: 7.1939...  0.0460 sec/batch\n",
      "Epoch: 7/20...  Training Step: 739...  Training loss: 7.1945...  0.1630 sec/batch\n",
      "Epoch: 7/20...  Training Step: 740...  Training loss: 7.1940...  0.3610 sec/batch\n",
      "Epoch: 7/20...  Training Step: 741...  Training loss: 7.1936...  0.0420 sec/batch\n",
      "Epoch: 7/20...  Training Step: 742...  Training loss: 7.1935...  0.0400 sec/batch\n",
      "Epoch: 7/20...  Training Step: 743...  Training loss: 7.1932...  0.0380 sec/batch\n",
      "Epoch: 7/20...  Training Step: 744...  Training loss: 7.1944...  0.0420 sec/batch\n",
      "Epoch: 7/20...  Training Step: 745...  Training loss: 7.1939...  0.0490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 746...  Training loss: 7.1943...  0.0480 sec/batch\n",
      "Epoch: 7/20...  Training Step: 747...  Training loss: 7.1942...  0.0590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 748...  Training loss: 7.1937...  0.0760 sec/batch\n",
      "Epoch: 7/20...  Training Step: 749...  Training loss: 7.1941...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 750...  Training loss: 7.1940...  0.0660 sec/batch\n",
      "Epoch: 7/20...  Training Step: 751...  Training loss: 7.1939...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 752...  Training loss: 7.1941...  0.0620 sec/batch\n",
      "Epoch: 7/20...  Training Step: 753...  Training loss: 7.1940...  0.0460 sec/batch\n",
      "Epoch: 7/20...  Training Step: 754...  Training loss: 7.1937...  0.0570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 755...  Training loss: 7.1933...  0.0740 sec/batch\n",
      "Epoch: 7/20...  Training Step: 756...  Training loss: 7.1938...  0.0480 sec/batch\n",
      "Epoch: 7/20...  Training Step: 757...  Training loss: 7.1935...  0.0550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 758...  Training loss: 7.1937...  0.0550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 759...  Training loss: 7.1935...  0.0540 sec/batch\n",
      "Epoch: 7/20...  Training Step: 760...  Training loss: 7.1933...  0.0560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 761...  Training loss: 7.1940...  0.0610 sec/batch\n",
      "Epoch: 7/20...  Training Step: 762...  Training loss: 7.1940...  0.0540 sec/batch\n",
      "Epoch: 7/20...  Training Step: 763...  Training loss: 7.1934...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 764...  Training loss: 7.1934...  0.0490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 765...  Training loss: 7.1939...  0.0610 sec/batch\n",
      "Epoch: 7/20...  Training Step: 766...  Training loss: 7.1940...  0.0810 sec/batch\n",
      "Epoch: 7/20...  Training Step: 767...  Training loss: 7.1937...  0.0980 sec/batch\n",
      "Epoch: 7/20...  Training Step: 768...  Training loss: 7.1945...  0.0650 sec/batch\n",
      "Epoch: 7/20...  Training Step: 769...  Training loss: 7.1943...  0.0570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 770...  Training loss: 7.1943...  0.0630 sec/batch\n",
      "Epoch: 7/20...  Training Step: 771...  Training loss: 7.1939...  0.0520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 772...  Training loss: 7.1942...  0.1000 sec/batch\n",
      "Epoch: 7/20...  Training Step: 773...  Training loss: 7.1936...  0.0850 sec/batch\n",
      "Epoch: 7/20...  Training Step: 774...  Training loss: 7.1941...  0.0690 sec/batch\n",
      "Epoch: 7/20...  Training Step: 775...  Training loss: 7.1934...  0.0560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 776...  Training loss: 7.1937...  0.0820 sec/batch\n",
      "Epoch: 7/20...  Training Step: 777...  Training loss: 7.1934...  0.0920 sec/batch\n",
      "Epoch: 7/20...  Training Step: 778...  Training loss: 7.1942...  0.0690 sec/batch\n",
      "Epoch: 7/20...  Training Step: 779...  Training loss: 7.1938...  0.0670 sec/batch\n",
      "Epoch: 7/20...  Training Step: 780...  Training loss: 7.1941...  0.2390 sec/batch\n",
      "Epoch: 7/20...  Training Step: 781...  Training loss: 7.1936...  0.0410 sec/batch\n",
      "Epoch: 7/20...  Training Step: 782...  Training loss: 7.1942...  0.0450 sec/batch\n",
      "Epoch: 7/20...  Training Step: 783...  Training loss: 7.1933...  0.0440 sec/batch\n",
      "Epoch: 7/20...  Training Step: 784...  Training loss: 7.1938...  0.0430 sec/batch\n",
      "Epoch: 7/20...  Training Step: 785...  Training loss: 7.1935...  0.0390 sec/batch\n",
      "Epoch: 7/20...  Training Step: 786...  Training loss: 7.1933...  0.0450 sec/batch\n",
      "Epoch: 7/20...  Training Step: 787...  Training loss: 7.1938...  0.0640 sec/batch\n",
      "Epoch: 7/20...  Training Step: 788...  Training loss: 7.1942...  0.0750 sec/batch\n",
      "Epoch: 7/20...  Training Step: 789...  Training loss: 7.1942...  0.0500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 790...  Training loss: 7.1936...  0.0530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 791...  Training loss: 7.1940...  0.1110 sec/batch\n",
      "Epoch: 7/20...  Training Step: 792...  Training loss: 7.1931...  0.0730 sec/batch\n",
      "Epoch: 7/20...  Training Step: 793...  Training loss: 7.1946...  0.0680 sec/batch\n",
      "Epoch: 7/20...  Training Step: 794...  Training loss: 7.1942...  0.0500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 795...  Training loss: 7.1940...  0.0600 sec/batch\n",
      "Epoch: 7/20...  Training Step: 796...  Training loss: 7.1940...  0.0620 sec/batch\n",
      "Epoch: 7/20...  Training Step: 797...  Training loss: 7.1933...  0.0810 sec/batch\n",
      "Epoch: 7/20...  Training Step: 798...  Training loss: 7.1931...  0.0630 sec/batch\n",
      "Epoch: 7/20...  Training Step: 799...  Training loss: 7.1941...  0.0450 sec/batch\n",
      "Epoch: 7/20...  Training Step: 800...  Training loss: 7.1932...  0.0770 sec/batch\n",
      "Epoch: 7/20...  Training Step: 801...  Training loss: 7.1939...  0.0440 sec/batch\n",
      "Epoch: 7/20...  Training Step: 802...  Training loss: 7.1936...  0.0380 sec/batch\n",
      "Epoch: 7/20...  Training Step: 803...  Training loss: 7.1943...  0.0450 sec/batch\n",
      "Epoch: 7/20...  Training Step: 804...  Training loss: 7.1947...  0.0400 sec/batch\n",
      "Epoch: 7/20...  Training Step: 805...  Training loss: 7.1929...  0.0420 sec/batch\n",
      "Epoch: 7/20...  Training Step: 806...  Training loss: 7.1934...  0.0430 sec/batch\n",
      "Epoch: 7/20...  Training Step: 807...  Training loss: 7.1942...  0.0670 sec/batch\n",
      "Epoch: 7/20...  Training Step: 808...  Training loss: 7.1934...  0.0550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 809...  Training loss: 7.1939...  0.0740 sec/batch\n",
      "Epoch: 7/20...  Training Step: 810...  Training loss: 7.1933...  0.0750 sec/batch\n",
      "Epoch: 7/20...  Training Step: 811...  Training loss: 7.1939...  0.0730 sec/batch\n",
      "Epoch: 7/20...  Training Step: 812...  Training loss: 7.1937...  0.1450 sec/batch\n",
      "Epoch: 7/20...  Training Step: 813...  Training loss: 7.1937...  0.1850 sec/batch\n",
      "Epoch: 7/20...  Training Step: 814...  Training loss: 7.1935...  0.0730 sec/batch\n",
      "Epoch: 7/20...  Training Step: 815...  Training loss: 7.1935...  0.1680 sec/batch\n",
      "Epoch: 7/20...  Training Step: 816...  Training loss: 7.1936...  0.0840 sec/batch\n",
      "Epoch: 7/20...  Training Step: 817...  Training loss: 7.1937...  0.0800 sec/batch\n",
      "Epoch: 7/20...  Training Step: 818...  Training loss: 7.1941...  0.0810 sec/batch\n",
      "Epoch: 7/20...  Training Step: 819...  Training loss: 7.1938...  0.0770 sec/batch\n",
      "Epoch: 8/20...  Training Step: 820...  Training loss: 7.1945...  0.0580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 821...  Training loss: 7.1935...  0.0400 sec/batch\n",
      "Epoch: 8/20...  Training Step: 822...  Training loss: 7.1940...  0.0400 sec/batch\n",
      "Epoch: 8/20...  Training Step: 823...  Training loss: 7.1933...  0.0360 sec/batch\n",
      "Epoch: 8/20...  Training Step: 824...  Training loss: 7.1938...  0.0450 sec/batch\n",
      "Epoch: 8/20...  Training Step: 825...  Training loss: 7.1942...  0.0420 sec/batch\n",
      "Epoch: 8/20...  Training Step: 826...  Training loss: 7.1938...  0.0430 sec/batch\n",
      "Epoch: 8/20...  Training Step: 827...  Training loss: 7.1938...  0.0680 sec/batch\n",
      "Epoch: 8/20...  Training Step: 828...  Training loss: 7.1940...  0.0530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 829...  Training loss: 7.1931...  0.0520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 830...  Training loss: 7.1951...  0.0490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 831...  Training loss: 7.1940...  0.0720 sec/batch\n",
      "Epoch: 8/20...  Training Step: 832...  Training loss: 7.1936...  0.0830 sec/batch\n",
      "Epoch: 8/20...  Training Step: 833...  Training loss: 7.1941...  0.0830 sec/batch\n",
      "Epoch: 8/20...  Training Step: 834...  Training loss: 7.1939...  0.0760 sec/batch\n",
      "Epoch: 8/20...  Training Step: 835...  Training loss: 7.1941...  0.0550 sec/batch\n",
      "Epoch: 8/20...  Training Step: 836...  Training loss: 7.1945...  0.0680 sec/batch\n",
      "Epoch: 8/20...  Training Step: 837...  Training loss: 7.1937...  0.0510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 838...  Training loss: 7.1935...  0.0560 sec/batch\n",
      "Epoch: 8/20...  Training Step: 839...  Training loss: 7.1931...  0.0440 sec/batch\n",
      "Epoch: 8/20...  Training Step: 840...  Training loss: 7.1939...  0.0440 sec/batch\n",
      "Epoch: 8/20...  Training Step: 841...  Training loss: 7.1938...  0.0400 sec/batch\n",
      "Epoch: 8/20...  Training Step: 842...  Training loss: 7.1937...  0.0450 sec/batch\n",
      "Epoch: 8/20...  Training Step: 843...  Training loss: 7.1934...  0.0380 sec/batch\n",
      "Epoch: 8/20...  Training Step: 844...  Training loss: 7.1937...  0.0430 sec/batch\n",
      "Epoch: 8/20...  Training Step: 845...  Training loss: 7.1940...  0.0520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 846...  Training loss: 7.1928...  0.0640 sec/batch\n",
      "Epoch: 8/20...  Training Step: 847...  Training loss: 7.1932...  0.0390 sec/batch\n",
      "Epoch: 8/20...  Training Step: 848...  Training loss: 7.1933...  0.0470 sec/batch\n",
      "Epoch: 8/20...  Training Step: 849...  Training loss: 7.1932...  0.0500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 850...  Training loss: 7.1941...  0.0710 sec/batch\n",
      "Epoch: 8/20...  Training Step: 851...  Training loss: 7.1941...  0.0420 sec/batch\n",
      "Epoch: 8/20...  Training Step: 852...  Training loss: 7.1938...  0.0460 sec/batch\n",
      "Epoch: 8/20...  Training Step: 853...  Training loss: 7.1930...  0.0510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 854...  Training loss: 7.1942...  0.0410 sec/batch\n",
      "Epoch: 8/20...  Training Step: 855...  Training loss: 7.1934...  0.0430 sec/batch\n",
      "Epoch: 8/20...  Training Step: 856...  Training loss: 7.1933...  0.0430 sec/batch\n",
      "Epoch: 8/20...  Training Step: 857...  Training loss: 7.1931...  0.0470 sec/batch\n",
      "Epoch: 8/20...  Training Step: 858...  Training loss: 7.1936...  0.0630 sec/batch\n",
      "Epoch: 8/20...  Training Step: 859...  Training loss: 7.1937...  0.0500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 860...  Training loss: 7.1928...  0.0670 sec/batch\n",
      "Epoch: 8/20...  Training Step: 861...  Training loss: 7.1936...  0.0400 sec/batch\n",
      "Epoch: 8/20...  Training Step: 862...  Training loss: 7.1937...  0.0460 sec/batch\n",
      "Epoch: 8/20...  Training Step: 863...  Training loss: 7.1935...  0.0430 sec/batch\n",
      "Epoch: 8/20...  Training Step: 864...  Training loss: 7.1938...  0.0430 sec/batch\n",
      "Epoch: 8/20...  Training Step: 865...  Training loss: 7.1935...  0.0460 sec/batch\n",
      "Epoch: 8/20...  Training Step: 866...  Training loss: 7.1935...  0.0420 sec/batch\n",
      "Epoch: 8/20...  Training Step: 867...  Training loss: 7.1940...  0.0530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 868...  Training loss: 7.1930...  0.0470 sec/batch\n",
      "Epoch: 8/20...  Training Step: 869...  Training loss: 7.1936...  0.0540 sec/batch\n",
      "Epoch: 8/20...  Training Step: 870...  Training loss: 7.1933...  0.0430 sec/batch\n",
      "Epoch: 8/20...  Training Step: 871...  Training loss: 7.1936...  0.0480 sec/batch\n",
      "Epoch: 8/20...  Training Step: 872...  Training loss: 7.1935...  0.0410 sec/batch\n",
      "Epoch: 8/20...  Training Step: 873...  Training loss: 7.1933...  0.0390 sec/batch\n",
      "Epoch: 8/20...  Training Step: 874...  Training loss: 7.1930...  0.0470 sec/batch\n",
      "Epoch: 8/20...  Training Step: 875...  Training loss: 7.1938...  0.0490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 876...  Training loss: 7.1934...  0.0490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 877...  Training loss: 7.1941...  0.0460 sec/batch\n",
      "Epoch: 8/20...  Training Step: 878...  Training loss: 7.1940...  0.0460 sec/batch\n",
      "Epoch: 8/20...  Training Step: 879...  Training loss: 7.1935...  0.0450 sec/batch\n",
      "Epoch: 8/20...  Training Step: 880...  Training loss: 7.1936...  0.0490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 881...  Training loss: 7.1936...  0.0530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 882...  Training loss: 7.1942...  0.0450 sec/batch\n",
      "Epoch: 8/20...  Training Step: 883...  Training loss: 7.1940...  0.0450 sec/batch\n",
      "Epoch: 8/20...  Training Step: 884...  Training loss: 7.1940...  0.0420 sec/batch\n",
      "Epoch: 8/20...  Training Step: 885...  Training loss: 7.1936...  0.0440 sec/batch\n",
      "Epoch: 8/20...  Training Step: 886...  Training loss: 7.1946...  0.0510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 887...  Training loss: 7.1930...  0.0590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 888...  Training loss: 7.1931...  0.0450 sec/batch\n",
      "Epoch: 8/20...  Training Step: 889...  Training loss: 7.1938...  0.0520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 890...  Training loss: 7.1927...  0.0540 sec/batch\n",
      "Epoch: 8/20...  Training Step: 891...  Training loss: 7.1941...  0.0550 sec/batch\n",
      "Epoch: 8/20...  Training Step: 892...  Training loss: 7.1931...  0.0520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 893...  Training loss: 7.1931...  0.0530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 894...  Training loss: 7.1938...  0.0450 sec/batch\n",
      "Epoch: 8/20...  Training Step: 895...  Training loss: 7.1935...  0.0520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 896...  Training loss: 7.1941...  0.0470 sec/batch\n",
      "Epoch: 8/20...  Training Step: 897...  Training loss: 7.1936...  0.0520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 898...  Training loss: 7.1929...  0.0410 sec/batch\n",
      "Epoch: 8/20...  Training Step: 899...  Training loss: 7.1940...  0.0530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 900...  Training loss: 7.1927...  0.0530 sec/batch\n",
      "Epoch: 8/20...  Training Step: 901...  Training loss: 7.1937...  0.0410 sec/batch\n",
      "Epoch: 8/20...  Training Step: 902...  Training loss: 7.1941...  0.0410 sec/batch\n",
      "Epoch: 8/20...  Training Step: 903...  Training loss: 7.1928...  0.0400 sec/batch\n",
      "Epoch: 8/20...  Training Step: 904...  Training loss: 7.1929...  0.0370 sec/batch\n",
      "Epoch: 8/20...  Training Step: 905...  Training loss: 7.1942...  0.0390 sec/batch\n",
      "Epoch: 8/20...  Training Step: 906...  Training loss: 7.1933...  0.0400 sec/batch\n",
      "Epoch: 8/20...  Training Step: 907...  Training loss: 7.1936...  0.0620 sec/batch\n",
      "Epoch: 8/20...  Training Step: 908...  Training loss: 7.1939...  0.0540 sec/batch\n",
      "Epoch: 8/20...  Training Step: 909...  Training loss: 7.1931...  0.0510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 910...  Training loss: 7.1940...  0.0410 sec/batch\n",
      "Epoch: 8/20...  Training Step: 911...  Training loss: 7.1929...  0.0420 sec/batch\n",
      "Epoch: 8/20...  Training Step: 912...  Training loss: 7.1936...  0.0430 sec/batch\n",
      "Epoch: 8/20...  Training Step: 913...  Training loss: 7.1928...  0.0410 sec/batch\n",
      "Epoch: 8/20...  Training Step: 914...  Training loss: 7.1935...  0.0590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 915...  Training loss: 7.1929...  0.0540 sec/batch\n",
      "Epoch: 8/20...  Training Step: 916...  Training loss: 7.1934...  0.0440 sec/batch\n",
      "Epoch: 8/20...  Training Step: 917...  Training loss: 7.1934...  0.0570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 918...  Training loss: 7.1929...  0.0450 sec/batch\n",
      "Epoch: 8/20...  Training Step: 919...  Training loss: 7.1937...  0.0650 sec/batch\n",
      "Epoch: 8/20...  Training Step: 920...  Training loss: 7.1930...  0.0500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 921...  Training loss: 7.1937...  0.0380 sec/batch\n",
      "Epoch: 8/20...  Training Step: 922...  Training loss: 7.1933...  0.0380 sec/batch\n",
      "Epoch: 8/20...  Training Step: 923...  Training loss: 7.1935...  0.0380 sec/batch\n",
      "Epoch: 8/20...  Training Step: 924...  Training loss: 7.1940...  0.0390 sec/batch\n",
      "Epoch: 8/20...  Training Step: 925...  Training loss: 7.1936...  0.0410 sec/batch\n",
      "Epoch: 8/20...  Training Step: 926...  Training loss: 7.1940...  0.0370 sec/batch\n",
      "Epoch: 8/20...  Training Step: 927...  Training loss: 7.1936...  0.0560 sec/batch\n",
      "Epoch: 8/20...  Training Step: 928...  Training loss: 7.1937...  0.0730 sec/batch\n",
      "Epoch: 8/20...  Training Step: 929...  Training loss: 7.1925...  0.0440 sec/batch\n",
      "Epoch: 8/20...  Training Step: 930...  Training loss: 7.1932...  0.0420 sec/batch\n",
      "Epoch: 8/20...  Training Step: 931...  Training loss: 7.1935...  0.0430 sec/batch\n",
      "Epoch: 8/20...  Training Step: 932...  Training loss: 7.1929...  0.0690 sec/batch\n",
      "Epoch: 8/20...  Training Step: 933...  Training loss: 7.1933...  0.0460 sec/batch\n",
      "Epoch: 8/20...  Training Step: 934...  Training loss: 7.1927...  0.0480 sec/batch\n",
      "Epoch: 8/20...  Training Step: 935...  Training loss: 7.1930...  0.0590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 936...  Training loss: 7.1934...  0.0460 sec/batch\n",
      "Epoch: 9/20...  Training Step: 937...  Training loss: 7.1940...  0.0420 sec/batch\n",
      "Epoch: 9/20...  Training Step: 938...  Training loss: 7.1933...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 939...  Training loss: 7.1943...  0.0440 sec/batch\n",
      "Epoch: 9/20...  Training Step: 940...  Training loss: 7.1929...  0.0520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 941...  Training loss: 7.1936...  0.0370 sec/batch\n",
      "Epoch: 9/20...  Training Step: 942...  Training loss: 7.1938...  0.0410 sec/batch\n",
      "Epoch: 9/20...  Training Step: 943...  Training loss: 7.1931...  0.0450 sec/batch\n",
      "Epoch: 9/20...  Training Step: 944...  Training loss: 7.1933...  0.0390 sec/batch\n",
      "Epoch: 9/20...  Training Step: 945...  Training loss: 7.1944...  0.0400 sec/batch\n",
      "Epoch: 9/20...  Training Step: 946...  Training loss: 7.1934...  0.0400 sec/batch\n",
      "Epoch: 9/20...  Training Step: 947...  Training loss: 7.1942...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 948...  Training loss: 7.1945...  0.0740 sec/batch\n",
      "Epoch: 9/20...  Training Step: 949...  Training loss: 7.1943...  0.0620 sec/batch\n",
      "Epoch: 9/20...  Training Step: 950...  Training loss: 7.1937...  0.0520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 951...  Training loss: 7.1933...  0.0560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 952...  Training loss: 7.1937...  0.0430 sec/batch\n",
      "Epoch: 9/20...  Training Step: 953...  Training loss: 7.1935...  0.0630 sec/batch\n",
      "Epoch: 9/20...  Training Step: 954...  Training loss: 7.1941...  0.0410 sec/batch\n",
      "Epoch: 9/20...  Training Step: 955...  Training loss: 7.1934...  0.0470 sec/batch\n",
      "Epoch: 9/20...  Training Step: 956...  Training loss: 7.1926...  0.0560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 957...  Training loss: 7.1933...  0.0510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 958...  Training loss: 7.1936...  0.0580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 959...  Training loss: 7.1933...  0.0490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 960...  Training loss: 7.1932...  0.0390 sec/batch\n",
      "Epoch: 9/20...  Training Step: 961...  Training loss: 7.1935...  0.0400 sec/batch\n",
      "Epoch: 9/20...  Training Step: 962...  Training loss: 7.1935...  0.0470 sec/batch\n",
      "Epoch: 9/20...  Training Step: 963...  Training loss: 7.1940...  0.0380 sec/batch\n",
      "Epoch: 9/20...  Training Step: 964...  Training loss: 7.1930...  0.0390 sec/batch\n",
      "Epoch: 9/20...  Training Step: 965...  Training loss: 7.1928...  0.0370 sec/batch\n",
      "Epoch: 9/20...  Training Step: 966...  Training loss: 7.1928...  0.0380 sec/batch\n",
      "Epoch: 9/20...  Training Step: 967...  Training loss: 7.1937...  0.0640 sec/batch\n",
      "Epoch: 9/20...  Training Step: 968...  Training loss: 7.1929...  0.0460 sec/batch\n",
      "Epoch: 9/20...  Training Step: 969...  Training loss: 7.1932...  0.0560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 970...  Training loss: 7.1928...  0.0390 sec/batch\n",
      "Epoch: 9/20...  Training Step: 971...  Training loss: 7.1937...  0.0420 sec/batch\n",
      "Epoch: 9/20...  Training Step: 972...  Training loss: 7.1929...  0.0550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 973...  Training loss: 7.1938...  0.0420 sec/batch\n",
      "Epoch: 9/20...  Training Step: 974...  Training loss: 7.1940...  0.0450 sec/batch\n",
      "Epoch: 9/20...  Training Step: 975...  Training loss: 7.1926...  0.0500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 976...  Training loss: 7.1934...  0.0470 sec/batch\n",
      "Epoch: 9/20...  Training Step: 977...  Training loss: 7.1932...  0.0600 sec/batch\n",
      "Epoch: 9/20...  Training Step: 978...  Training loss: 7.1935...  0.0440 sec/batch\n",
      "Epoch: 9/20...  Training Step: 979...  Training loss: 7.1937...  0.0420 sec/batch\n",
      "Epoch: 9/20...  Training Step: 980...  Training loss: 7.1932...  0.0460 sec/batch\n",
      "Epoch: 9/20...  Training Step: 981...  Training loss: 7.1937...  0.0430 sec/batch\n",
      "Epoch: 9/20...  Training Step: 982...  Training loss: 7.1930...  0.0530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 983...  Training loss: 7.1931...  0.0400 sec/batch\n",
      "Epoch: 9/20...  Training Step: 984...  Training loss: 7.1937...  0.0460 sec/batch\n",
      "Epoch: 9/20...  Training Step: 985...  Training loss: 7.1929...  0.0530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 986...  Training loss: 7.1934...  0.0630 sec/batch\n",
      "Epoch: 9/20...  Training Step: 987...  Training loss: 7.1925...  0.0660 sec/batch\n",
      "Epoch: 9/20...  Training Step: 988...  Training loss: 7.1938...  0.0490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 989...  Training loss: 7.1935...  0.0450 sec/batch\n",
      "Epoch: 9/20...  Training Step: 990...  Training loss: 7.1932...  0.0570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 991...  Training loss: 7.1934...  0.0730 sec/batch\n",
      "Epoch: 9/20...  Training Step: 992...  Training loss: 7.1934...  0.0570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 993...  Training loss: 7.1927...  0.0670 sec/batch\n",
      "Epoch: 9/20...  Training Step: 994...  Training loss: 7.1934...  0.0700 sec/batch\n",
      "Epoch: 9/20...  Training Step: 995...  Training loss: 7.1934...  0.0570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 996...  Training loss: 7.1927...  0.0430 sec/batch\n",
      "Epoch: 9/20...  Training Step: 997...  Training loss: 7.1924...  0.0510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 998...  Training loss: 7.1933...  0.0540 sec/batch\n",
      "Epoch: 9/20...  Training Step: 999...  Training loss: 7.1930...  0.0470 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1000...  Training loss: 7.1927...  0.0390 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1001...  Training loss: 7.1933...  0.0360 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1002...  Training loss: 7.1936...  0.0370 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1003...  Training loss: 7.1945...  0.0370 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1004...  Training loss: 7.1937...  0.0360 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1005...  Training loss: 7.1928...  0.0390 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1006...  Training loss: 7.1937...  0.0380 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1007...  Training loss: 7.1929...  0.0540 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1008...  Training loss: 7.1931...  0.0650 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1009...  Training loss: 7.1932...  0.0380 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1010...  Training loss: 7.1933...  0.0360 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1011...  Training loss: 7.1932...  0.0560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1012...  Training loss: 7.1936...  0.0530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1013...  Training loss: 7.1929...  0.0480 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1014...  Training loss: 7.1929...  0.0430 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1015...  Training loss: 7.1928...  0.0430 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1016...  Training loss: 7.1932...  0.0500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1017...  Training loss: 7.1932...  0.0500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1018...  Training loss: 7.1928...  0.0690 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1019...  Training loss: 7.1933...  0.0520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1020...  Training loss: 7.1923...  0.0490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1021...  Training loss: 7.1930...  0.0410 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1022...  Training loss: 7.1930...  0.0440 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1023...  Training loss: 7.1928...  0.0460 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1024...  Training loss: 7.1929...  0.0490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1025...  Training loss: 7.1933...  0.0460 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1026...  Training loss: 7.1929...  0.0670 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1027...  Training loss: 7.1931...  0.0730 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1028...  Training loss: 7.1921...  0.0580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1029...  Training loss: 7.1933...  0.0430 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1030...  Training loss: 7.1924...  0.0820 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1031...  Training loss: 7.1928...  0.0620 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1032...  Training loss: 7.1925...  0.0590 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1033...  Training loss: 7.1931...  0.0580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1034...  Training loss: 7.1930...  0.0570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1035...  Training loss: 7.1924...  0.0440 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1036...  Training loss: 7.1933...  0.0500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1037...  Training loss: 7.1931...  0.0620 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1038...  Training loss: 7.1939...  0.0760 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1039...  Training loss: 7.1929...  0.0440 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1040...  Training loss: 7.1925...  0.0500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1041...  Training loss: 7.1936...  0.0540 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1042...  Training loss: 7.1923...  0.0490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1043...  Training loss: 7.1941...  0.0400 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1044...  Training loss: 7.1932...  0.0400 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1045...  Training loss: 7.1939...  0.0420 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1046...  Training loss: 7.1926...  0.0510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1047...  Training loss: 7.1931...  0.0640 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1048...  Training loss: 7.1923...  0.0430 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1049...  Training loss: 7.1932...  0.0810 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1050...  Training loss: 7.1932...  0.0700 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1051...  Training loss: 7.1920...  0.0470 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1052...  Training loss: 7.1935...  0.0430 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1053...  Training loss: 7.1932...  0.0610 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1054...  Training loss: 7.1934...  0.0460 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1055...  Training loss: 7.1931...  0.0480 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1056...  Training loss: 7.1942...  0.0510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1057...  Training loss: 7.1922...  0.0500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1058...  Training loss: 7.1923...  0.0430 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1059...  Training loss: 7.1929...  0.0520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1060...  Training loss: 7.1929...  0.0650 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1061...  Training loss: 7.1932...  0.0390 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1062...  Training loss: 7.1954...  0.0440 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1063...  Training loss: 7.1930...  0.0400 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1064...  Training loss: 7.1940...  0.0450 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1065...  Training loss: 7.1936...  0.0410 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1066...  Training loss: 7.1934...  0.0460 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1067...  Training loss: 7.1931...  0.0600 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1068...  Training loss: 7.1930...  0.0610 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1069...  Training loss: 7.1930...  0.0510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1070...  Training loss: 7.1927...  0.0560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1071...  Training loss: 7.1931...  0.0400 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1072...  Training loss: 7.1931...  0.0640 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1073...  Training loss: 7.1927...  0.0430 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1074...  Training loss: 7.1929...  0.0500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1075...  Training loss: 7.1930...  0.0540 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1076...  Training loss: 7.1928...  0.0620 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1077...  Training loss: 7.1933...  0.0440 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1078...  Training loss: 7.1929...  0.0560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1079...  Training loss: 7.1931...  0.0610 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1080...  Training loss: 7.1929...  0.0440 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1081...  Training loss: 7.1922...  0.0360 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1082...  Training loss: 7.1941...  0.0440 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1083...  Training loss: 7.1921...  0.0380 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1084...  Training loss: 7.1934...  0.0390 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1085...  Training loss: 7.1923...  0.0410 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1086...  Training loss: 7.1938...  0.0380 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1087...  Training loss: 7.1915...  0.0620 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1088...  Training loss: 7.1940...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1089...  Training loss: 7.1933...  0.0620 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1090...  Training loss: 7.1930...  0.0410 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1091...  Training loss: 7.1934...  0.0380 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1092...  Training loss: 7.1927...  0.0630 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1093...  Training loss: 7.1926...  0.0540 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1094...  Training loss: 7.1925...  0.0400 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1095...  Training loss: 7.1933...  0.0500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1096...  Training loss: 7.1924...  0.0400 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1097...  Training loss: 7.1931...  0.0670 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1098...  Training loss: 7.1944...  0.0410 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1099...  Training loss: 7.1922...  0.0450 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1100...  Training loss: 7.1928...  0.0450 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1101...  Training loss: 7.1932...  0.0390 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1102...  Training loss: 7.1924...  0.0380 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1103...  Training loss: 7.1936...  0.0380 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1104...  Training loss: 7.1932...  0.0410 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1105...  Training loss: 7.1930...  0.0360 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1106...  Training loss: 7.1925...  0.0470 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1107...  Training loss: 7.1928...  0.0590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1108...  Training loss: 7.1934...  0.0620 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1109...  Training loss: 7.1932...  0.0760 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1110...  Training loss: 7.1930...  0.0460 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1111...  Training loss: 7.1936...  0.0420 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1112...  Training loss: 7.1943...  0.0550 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1113...  Training loss: 7.1926...  0.0680 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1114...  Training loss: 7.1929...  0.0560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1115...  Training loss: 7.1924...  0.0460 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1116...  Training loss: 7.1922...  0.0560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1117...  Training loss: 7.1936...  0.0450 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1118...  Training loss: 7.1924...  0.0540 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1119...  Training loss: 7.1935...  0.0450 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1120...  Training loss: 7.1934...  0.0580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1121...  Training loss: 7.1921...  0.0390 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1122...  Training loss: 7.1915...  0.0410 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1123...  Training loss: 7.1935...  0.0370 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1124...  Training loss: 7.1926...  0.0380 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1125...  Training loss: 7.1929...  0.0380 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1126...  Training loss: 7.1932...  0.0400 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1127...  Training loss: 7.1921...  0.0540 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1128...  Training loss: 7.1940...  0.0430 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1129...  Training loss: 7.1936...  0.0440 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1130...  Training loss: 7.1924...  0.0410 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1131...  Training loss: 7.1931...  0.0460 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1132...  Training loss: 7.1924...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1133...  Training loss: 7.1932...  0.0430 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1134...  Training loss: 7.1925...  0.0520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1135...  Training loss: 7.1928...  0.0460 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1136...  Training loss: 7.1944...  0.0680 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1137...  Training loss: 7.1924...  0.0520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1138...  Training loss: 7.1933...  0.0390 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1139...  Training loss: 7.1939...  0.0440 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1140...  Training loss: 7.1932...  0.0400 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1141...  Training loss: 7.1930...  0.0360 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1142...  Training loss: 7.1925...  0.0370 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1143...  Training loss: 7.1931...  0.0380 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1144...  Training loss: 7.1935...  0.0360 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1145...  Training loss: 7.1920...  0.0380 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1146...  Training loss: 7.1924...  0.0360 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1147...  Training loss: 7.1926...  0.0540 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1148...  Training loss: 7.1933...  0.0620 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1149...  Training loss: 7.1932...  0.0370 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1150...  Training loss: 7.1926...  0.0560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1151...  Training loss: 7.1931...  0.0700 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1152...  Training loss: 7.1924...  0.0490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1153...  Training loss: 7.1930...  0.0470 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1154...  Training loss: 7.1930...  0.0520 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1155...  Training loss: 7.1929...  0.0560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1156...  Training loss: 7.1923...  0.0410 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1157...  Training loss: 7.1934...  0.0530 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1158...  Training loss: 7.1937...  0.0450 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1159...  Training loss: 7.1919...  0.0510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1160...  Training loss: 7.1933...  0.0510 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1161...  Training loss: 7.1932...  0.0370 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1162...  Training loss: 7.1933...  0.0420 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1163...  Training loss: 7.1920...  0.0390 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1164...  Training loss: 7.1925...  0.0380 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1165...  Training loss: 7.1916...  0.0420 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1166...  Training loss: 7.1925...  0.0400 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1167...  Training loss: 7.1932...  0.0560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1168...  Training loss: 7.1917...  0.0450 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1169...  Training loss: 7.1928...  0.0370 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1170...  Training loss: 7.1929...  0.0580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1171...  Training loss: 7.1935...  0.0560 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1172...  Training loss: 7.1933...  0.0520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1173...  Training loss: 7.1929...  0.0380 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1174...  Training loss: 7.1921...  0.0390 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1175...  Training loss: 7.1929...  0.0480 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1176...  Training loss: 7.1935...  0.0500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1177...  Training loss: 7.1924...  0.0440 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1178...  Training loss: 7.1934...  0.0400 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1179...  Training loss: 7.1940...  0.0440 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1180...  Training loss: 7.1931...  0.0400 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1181...  Training loss: 7.1939...  0.0380 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1182...  Training loss: 7.1938...  0.0420 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1183...  Training loss: 7.1934...  0.0510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1184...  Training loss: 7.1926...  0.0400 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1185...  Training loss: 7.1915...  0.0460 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1186...  Training loss: 7.1934...  0.0690 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1187...  Training loss: 7.1922...  0.0680 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1188...  Training loss: 7.1938...  0.0520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1189...  Training loss: 7.1921...  0.0500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1190...  Training loss: 7.1924...  0.0590 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1191...  Training loss: 7.1927...  0.0540 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1192...  Training loss: 7.1931...  0.0640 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1193...  Training loss: 7.1934...  0.0620 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1194...  Training loss: 7.1918...  0.0470 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1195...  Training loss: 7.1931...  0.0390 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1196...  Training loss: 7.1934...  0.0520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1197...  Training loss: 7.1919...  0.0660 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1198...  Training loss: 7.1914...  0.0430 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1199...  Training loss: 7.1930...  0.0540 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1200...  Training loss: 7.1919...  0.0480 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1201...  Training loss: 7.1933...  0.0430 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1202...  Training loss: 7.1921...  0.0400 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1203...  Training loss: 7.1936...  0.0400 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1204...  Training loss: 7.1923...  0.0460 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1205...  Training loss: 7.1933...  0.0390 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1206...  Training loss: 7.1924...  0.0400 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1207...  Training loss: 7.1929...  0.0580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1208...  Training loss: 7.1935...  0.0480 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1209...  Training loss: 7.1919...  0.1310 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1210...  Training loss: 7.1924...  0.0450 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1211...  Training loss: 7.1924...  0.0980 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1212...  Training loss: 7.1921...  0.0430 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1213...  Training loss: 7.1927...  0.0770 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1214...  Training loss: 7.1929...  0.0690 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1215...  Training loss: 7.1930...  0.0590 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1216...  Training loss: 7.1926...  0.0530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1217...  Training loss: 7.1935...  0.0590 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1218...  Training loss: 7.1924...  0.0510 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1219...  Training loss: 7.1926...  0.0410 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1220...  Training loss: 7.1928...  0.0700 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1221...  Training loss: 7.1923...  0.0390 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1222...  Training loss: 7.1925...  0.0500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1223...  Training loss: 7.1919...  0.0450 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1224...  Training loss: 7.1920...  0.0470 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1225...  Training loss: 7.1916...  0.0390 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1226...  Training loss: 7.1922...  0.0540 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1227...  Training loss: 7.1916...  0.0830 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1228...  Training loss: 7.1930...  0.0570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1229...  Training loss: 7.1933...  0.0700 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1230...  Training loss: 7.1922...  0.0470 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1231...  Training loss: 7.1931...  0.0490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1232...  Training loss: 7.1919...  0.0500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1233...  Training loss: 7.1916...  0.0430 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1234...  Training loss: 7.1932...  0.0570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1235...  Training loss: 7.1924...  0.0470 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1236...  Training loss: 7.1933...  0.0400 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1237...  Training loss: 7.1919...  0.0710 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1238...  Training loss: 7.1922...  0.0480 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1239...  Training loss: 7.1922...  0.0430 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1240...  Training loss: 7.1929...  0.0490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1241...  Training loss: 7.1922...  0.0370 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1242...  Training loss: 7.1923...  0.0380 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1243...  Training loss: 7.1925...  0.0380 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1244...  Training loss: 7.1936...  0.0370 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1245...  Training loss: 7.1927...  0.0390 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1246...  Training loss: 7.1930...  0.0460 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1247...  Training loss: 7.1925...  0.0620 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1248...  Training loss: 7.1924...  0.0530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1249...  Training loss: 7.1921...  0.0390 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1250...  Training loss: 7.1929...  0.0520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1251...  Training loss: 7.1915...  0.0540 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1252...  Training loss: 7.1926...  0.0480 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1253...  Training loss: 7.1928...  0.0530 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1254...  Training loss: 7.1920...  0.0430 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1255...  Training loss: 7.1923...  0.0540 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1256...  Training loss: 7.1930...  0.0470 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1257...  Training loss: 7.1922...  0.0580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1258...  Training loss: 7.1916...  0.0650 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1259...  Training loss: 7.1927...  0.0400 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1260...  Training loss: 7.1917...  0.0490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1261...  Training loss: 7.1925...  0.0470 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1262...  Training loss: 7.1914...  0.0420 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1263...  Training loss: 7.1931...  0.0430 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1264...  Training loss: 7.1924...  0.0370 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1265...  Training loss: 7.1925...  0.0380 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1266...  Training loss: 7.1928...  0.0410 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1267...  Training loss: 7.1920...  0.0650 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1268...  Training loss: 7.1924...  0.0380 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1269...  Training loss: 7.1909...  0.0400 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1270...  Training loss: 7.1926...  0.0470 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1271...  Training loss: 7.1928...  0.0630 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1272...  Training loss: 7.1930...  0.0550 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1273...  Training loss: 7.1926...  0.0490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1274...  Training loss: 7.1922...  0.0500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1275...  Training loss: 7.1928...  0.0420 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1276...  Training loss: 7.1919...  0.0540 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1277...  Training loss: 7.1938...  0.0440 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1278...  Training loss: 7.1923...  0.0660 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1279...  Training loss: 7.1929...  0.0520 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1280...  Training loss: 7.1911...  0.0650 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1281...  Training loss: 7.1934...  0.0370 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1282...  Training loss: 7.1923...  0.0460 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1283...  Training loss: 7.1908...  0.0390 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1284...  Training loss: 7.1912...  0.0380 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1285...  Training loss: 7.1917...  0.0370 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1286...  Training loss: 7.1929...  0.0380 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1287...  Training loss: 7.1929...  0.0620 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1288...  Training loss: 7.1930...  0.0410 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1289...  Training loss: 7.1923...  0.0560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1290...  Training loss: 7.1937...  0.0570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1291...  Training loss: 7.1926...  0.0470 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1292...  Training loss: 7.1922...  0.0840 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1293...  Training loss: 7.1932...  0.0510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1294...  Training loss: 7.1927...  0.0960 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1295...  Training loss: 7.1922...  0.0520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1296...  Training loss: 7.1938...  0.0530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1297...  Training loss: 7.1926...  0.0690 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1298...  Training loss: 7.1935...  0.0470 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1299...  Training loss: 7.1917...  0.0710 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1300...  Training loss: 7.1925...  0.0540 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1301...  Training loss: 7.1927...  0.0380 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1302...  Training loss: 7.1921...  0.0380 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1303...  Training loss: 7.1925...  0.0380 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1304...  Training loss: 7.1924...  0.0420 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1305...  Training loss: 7.1929...  0.0390 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1306...  Training loss: 7.1925...  0.0380 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1307...  Training loss: 7.1922...  0.0560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1308...  Training loss: 7.1922...  0.0520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1309...  Training loss: 7.1925...  0.0440 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1310...  Training loss: 7.1927...  0.0800 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1311...  Training loss: 7.1919...  0.0640 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1312...  Training loss: 7.1926...  0.0530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1313...  Training loss: 7.1919...  0.0480 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1314...  Training loss: 7.1927...  0.0420 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1315...  Training loss: 7.1915...  0.0440 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1316...  Training loss: 7.1925...  0.0450 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1317...  Training loss: 7.1919...  0.0470 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1318...  Training loss: 7.1921...  0.0540 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1319...  Training loss: 7.1919...  0.0520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1320...  Training loss: 7.1923...  0.0450 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1321...  Training loss: 7.1927...  0.0430 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1322...  Training loss: 7.1930...  0.0460 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1323...  Training loss: 7.1923...  0.0380 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1324...  Training loss: 7.1930...  0.0410 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1325...  Training loss: 7.1922...  0.0420 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1326...  Training loss: 7.1927...  0.0570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1327...  Training loss: 7.1928...  0.0610 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1328...  Training loss: 7.1911...  0.0450 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1329...  Training loss: 7.1916...  0.0620 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1330...  Training loss: 7.1913...  0.0610 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1331...  Training loss: 7.1926...  0.0710 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1332...  Training loss: 7.1931...  0.0590 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1333...  Training loss: 7.1913...  0.0770 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1334...  Training loss: 7.1913...  0.0670 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1335...  Training loss: 7.1931...  0.0610 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1336...  Training loss: 7.1917...  0.0540 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1337...  Training loss: 7.1936...  0.0470 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1338...  Training loss: 7.1914...  0.0560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1339...  Training loss: 7.1925...  0.0630 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1340...  Training loss: 7.1918...  0.0450 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1341...  Training loss: 7.1925...  0.0430 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1342...  Training loss: 7.1922...  0.0380 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1343...  Training loss: 7.1917...  0.0400 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1344...  Training loss: 7.1906...  0.0440 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1345...  Training loss: 7.1926...  0.0370 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1346...  Training loss: 7.1922...  0.0380 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1347...  Training loss: 7.1917...  0.0670 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1348...  Training loss: 7.1922...  0.0480 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1349...  Training loss: 7.1922...  0.0450 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1350...  Training loss: 7.1924...  0.0630 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1351...  Training loss: 7.1928...  0.0880 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1352...  Training loss: 7.1919...  0.0700 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1353...  Training loss: 7.1937...  0.0700 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1354...  Training loss: 7.1930...  0.0700 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1355...  Training loss: 7.1927...  0.0520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1356...  Training loss: 7.1916...  0.0500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1357...  Training loss: 7.1930...  0.0500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1358...  Training loss: 7.1918...  0.0590 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1359...  Training loss: 7.1926...  0.0620 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1360...  Training loss: 7.1922...  0.0740 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1361...  Training loss: 7.1923...  0.0370 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1362...  Training loss: 7.1935...  0.0420 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1363...  Training loss: 7.1928...  0.0370 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1364...  Training loss: 7.1921...  0.0400 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1365...  Training loss: 7.1925...  0.0470 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1366...  Training loss: 7.1918...  0.0420 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1367...  Training loss: 7.1920...  0.0670 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1368...  Training loss: 7.1916...  0.0480 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1369...  Training loss: 7.1924...  0.0560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1370...  Training loss: 7.1938...  0.0560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1371...  Training loss: 7.1914...  0.0400 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1372...  Training loss: 7.1925...  0.0560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1373...  Training loss: 7.1930...  0.0660 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1374...  Training loss: 7.1925...  0.0430 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1375...  Training loss: 7.1921...  0.0420 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1376...  Training loss: 7.1929...  0.0430 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1377...  Training loss: 7.1913...  0.0510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1378...  Training loss: 7.1916...  0.0420 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1379...  Training loss: 7.1920...  0.0510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1380...  Training loss: 7.1920...  0.0420 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1381...  Training loss: 7.1918...  0.0360 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1382...  Training loss: 7.1914...  0.0370 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1383...  Training loss: 7.1917...  0.0420 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1384...  Training loss: 7.1924...  0.0360 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1385...  Training loss: 7.1923...  0.0390 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1386...  Training loss: 7.1915...  0.0370 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1387...  Training loss: 7.1932...  0.0500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1388...  Training loss: 7.1931...  0.0470 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1389...  Training loss: 7.1926...  0.0560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1390...  Training loss: 7.1924...  0.0630 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1391...  Training loss: 7.1908...  0.0410 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1392...  Training loss: 7.1925...  0.0580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1393...  Training loss: 7.1906...  0.0520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1394...  Training loss: 7.1924...  0.0520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1395...  Training loss: 7.1933...  0.0480 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1396...  Training loss: 7.1921...  0.0470 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1397...  Training loss: 7.1917...  0.0470 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1398...  Training loss: 7.1922...  0.0560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1399...  Training loss: 7.1908...  0.0420 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1400...  Training loss: 7.1911...  0.0410 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1401...  Training loss: 7.1922...  0.0510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1402...  Training loss: 7.1907...  0.0520 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1403...  Training loss: 7.1918...  0.0380 sec/batch\n",
      "Epoch: 12/20...  Training Step: 1404...  Training loss: 7.1917...  0.0450 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1405...  Training loss: 7.1934...  0.0360 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1406...  Training loss: 7.1922...  0.0470 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1407...  Training loss: 7.1924...  0.0470 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1408...  Training loss: 7.1907...  0.0460 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1409...  Training loss: 7.1926...  0.0450 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1410...  Training loss: 7.1932...  0.0460 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1411...  Training loss: 7.1917...  0.0430 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1412...  Training loss: 7.1914...  0.0590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1413...  Training loss: 7.1939...  0.0690 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1414...  Training loss: 7.1910...  0.0440 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1415...  Training loss: 7.1940...  0.0400 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1416...  Training loss: 7.1924...  0.0560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1417...  Training loss: 7.1917...  0.0580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1418...  Training loss: 7.1926...  0.0580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1419...  Training loss: 7.1921...  0.0440 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1420...  Training loss: 7.1924...  0.0590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1421...  Training loss: 7.1929...  0.0420 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1422...  Training loss: 7.1918...  0.0390 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1423...  Training loss: 7.1920...  0.0520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1424...  Training loss: 7.1905...  0.0390 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1425...  Training loss: 7.1928...  0.0410 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1426...  Training loss: 7.1925...  0.0470 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1427...  Training loss: 7.1929...  0.0600 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1428...  Training loss: 7.1919...  0.0490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1429...  Training loss: 7.1917...  0.0460 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1430...  Training loss: 7.1923...  0.0660 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1431...  Training loss: 7.1914...  0.0510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1432...  Training loss: 7.1920...  0.0480 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1433...  Training loss: 7.1916...  0.0540 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1434...  Training loss: 7.1915...  0.0560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1435...  Training loss: 7.1914...  0.0550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1436...  Training loss: 7.1922...  0.0610 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1437...  Training loss: 7.1927...  0.0550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1438...  Training loss: 7.1922...  0.0760 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1439...  Training loss: 7.1925...  0.0510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1440...  Training loss: 7.1915...  0.0820 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1441...  Training loss: 7.1923...  0.0460 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1442...  Training loss: 7.1914...  0.0450 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1443...  Training loss: 7.1916...  0.0440 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1444...  Training loss: 7.1911...  0.0380 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1445...  Training loss: 7.1908...  0.0370 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1446...  Training loss: 7.1925...  0.0400 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1447...  Training loss: 7.1920...  0.0660 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1448...  Training loss: 7.1915...  0.0590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1449...  Training loss: 7.1928...  0.0580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1450...  Training loss: 7.1916...  0.0510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1451...  Training loss: 7.1907...  0.0510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1452...  Training loss: 7.1921...  0.0460 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1453...  Training loss: 7.1919...  0.0430 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1454...  Training loss: 7.1919...  0.0540 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1455...  Training loss: 7.1910...  0.0510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1456...  Training loss: 7.1920...  0.0490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1457...  Training loss: 7.1920...  0.0370 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1458...  Training loss: 7.1913...  0.0490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1459...  Training loss: 7.1908...  0.0610 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1460...  Training loss: 7.1925...  0.0560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1461...  Training loss: 7.1910...  0.0400 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1462...  Training loss: 7.1919...  0.0470 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1463...  Training loss: 7.1924...  0.0630 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1464...  Training loss: 7.1905...  0.0510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1465...  Training loss: 7.1911...  0.0590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1466...  Training loss: 7.1911...  0.0860 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1467...  Training loss: 7.1915...  0.0700 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1468...  Training loss: 7.1919...  0.0540 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1469...  Training loss: 7.1917...  0.0470 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1470...  Training loss: 7.1919...  0.0800 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1471...  Training loss: 7.1912...  0.0590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1472...  Training loss: 7.1931...  0.0510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1473...  Training loss: 7.1922...  0.0550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1474...  Training loss: 7.1917...  0.0400 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1475...  Training loss: 7.1906...  0.0740 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1476...  Training loss: 7.1922...  0.0650 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1477...  Training loss: 7.1911...  0.0930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1478...  Training loss: 7.1912...  0.0880 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1479...  Training loss: 7.1923...  0.0480 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1480...  Training loss: 7.1931...  0.0540 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1481...  Training loss: 7.1917...  0.0370 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1482...  Training loss: 7.1915...  0.0510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1483...  Training loss: 7.1907...  0.0380 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1484...  Training loss: 7.1919...  0.0440 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1485...  Training loss: 7.1898...  0.0370 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1486...  Training loss: 7.1917...  0.0450 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1487...  Training loss: 7.1919...  0.0620 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1488...  Training loss: 7.1914...  0.0500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1489...  Training loss: 7.1915...  0.0520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1490...  Training loss: 7.1917...  0.0430 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1491...  Training loss: 7.1918...  0.0490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1492...  Training loss: 7.1913...  0.0570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1493...  Training loss: 7.1919...  0.0410 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1494...  Training loss: 7.1911...  0.0640 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1495...  Training loss: 7.1922...  0.0410 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1496...  Training loss: 7.1917...  0.0470 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1497...  Training loss: 7.1921...  0.0420 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1498...  Training loss: 7.1899...  0.0610 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1499...  Training loss: 7.1909...  0.0490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1500...  Training loss: 7.1901...  0.0480 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1501...  Training loss: 7.1904...  0.0440 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1502...  Training loss: 7.1920...  0.0410 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1503...  Training loss: 7.1907...  0.0390 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1504...  Training loss: 7.1909...  0.0430 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1505...  Training loss: 7.1914...  0.0400 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1506...  Training loss: 7.1918...  0.0510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1507...  Training loss: 7.1914...  0.0560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1508...  Training loss: 7.1910...  0.0740 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1509...  Training loss: 7.1926...  0.0530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1510...  Training loss: 7.1900...  0.0810 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1511...  Training loss: 7.1921...  0.0540 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1512...  Training loss: 7.1912...  0.0390 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1513...  Training loss: 7.1932...  0.0480 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1514...  Training loss: 7.1894...  0.0680 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1515...  Training loss: 7.1923...  0.0510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1516...  Training loss: 7.1904...  0.0510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1517...  Training loss: 7.1909...  0.0600 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1518...  Training loss: 7.1922...  0.0450 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1519...  Training loss: 7.1909...  0.0400 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1520...  Training loss: 7.1912...  0.0570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 1521...  Training loss: 7.1912...  0.0420 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1522...  Training loss: 7.1919...  0.0410 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1523...  Training loss: 7.1917...  0.0470 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1524...  Training loss: 7.1922...  0.0390 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1525...  Training loss: 7.1907...  0.0450 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1526...  Training loss: 7.1922...  0.0700 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1527...  Training loss: 7.1928...  0.0400 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1528...  Training loss: 7.1918...  0.0500 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1529...  Training loss: 7.1912...  0.0490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1530...  Training loss: 7.1938...  0.0520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1531...  Training loss: 7.1920...  0.0590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1532...  Training loss: 7.1943...  0.0460 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1533...  Training loss: 7.1915...  0.0520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1534...  Training loss: 7.1916...  0.0410 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1535...  Training loss: 7.1915...  0.0450 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1536...  Training loss: 7.1912...  0.0700 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1537...  Training loss: 7.1928...  0.0550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1538...  Training loss: 7.1929...  0.0610 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1539...  Training loss: 7.1915...  0.0440 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1540...  Training loss: 7.1912...  0.0620 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1541...  Training loss: 7.1915...  0.0420 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1542...  Training loss: 7.1922...  0.0400 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1543...  Training loss: 7.1923...  0.0370 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1544...  Training loss: 7.1924...  0.0400 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1545...  Training loss: 7.1906...  0.0370 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1546...  Training loss: 7.1911...  0.0380 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1547...  Training loss: 7.1925...  0.0550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1548...  Training loss: 7.1920...  0.0390 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1549...  Training loss: 7.1918...  0.0390 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1550...  Training loss: 7.1913...  0.0520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1551...  Training loss: 7.1912...  0.0520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1552...  Training loss: 7.1918...  0.0410 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1553...  Training loss: 7.1922...  0.0540 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1554...  Training loss: 7.1931...  0.0380 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1555...  Training loss: 7.1906...  0.0480 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1556...  Training loss: 7.1914...  0.0570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1557...  Training loss: 7.1894...  0.0410 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1558...  Training loss: 7.1914...  0.0530 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1559...  Training loss: 7.1912...  0.0570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1560...  Training loss: 7.1911...  0.0740 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1561...  Training loss: 7.1910...  0.0410 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1562...  Training loss: 7.1909...  0.0430 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1563...  Training loss: 7.1904...  0.0440 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1564...  Training loss: 7.1899...  0.0420 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1565...  Training loss: 7.1922...  0.0400 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1566...  Training loss: 7.1922...  0.0430 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1567...  Training loss: 7.1912...  0.0680 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1568...  Training loss: 7.1910...  0.0610 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1569...  Training loss: 7.1918...  0.0440 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1570...  Training loss: 7.1907...  0.0400 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1571...  Training loss: 7.1920...  0.0600 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1572...  Training loss: 7.1918...  0.0580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1573...  Training loss: 7.1904...  0.0430 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1574...  Training loss: 7.1920...  0.0430 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1575...  Training loss: 7.1906...  0.0400 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1576...  Training loss: 7.1916...  0.0780 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1577...  Training loss: 7.1917...  0.0510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1578...  Training loss: 7.1908...  0.0540 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1579...  Training loss: 7.1919...  0.0480 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1580...  Training loss: 7.1923...  0.0650 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1581...  Training loss: 7.1908...  0.0370 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1582...  Training loss: 7.1914...  0.0410 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1583...  Training loss: 7.1917...  0.0370 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1584...  Training loss: 7.1911...  0.0400 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1585...  Training loss: 7.1902...  0.0430 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1586...  Training loss: 7.1920...  0.0370 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1587...  Training loss: 7.1913...  0.0600 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1588...  Training loss: 7.1923...  0.0600 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1589...  Training loss: 7.1913...  0.0550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1590...  Training loss: 7.1919...  0.0460 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1591...  Training loss: 7.1915...  0.0430 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1592...  Training loss: 7.1907...  0.0580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1593...  Training loss: 7.1918...  0.0390 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1594...  Training loss: 7.1913...  0.0480 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1595...  Training loss: 7.1916...  0.0390 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1596...  Training loss: 7.1930...  0.0450 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1597...  Training loss: 7.1915...  0.0480 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1598...  Training loss: 7.1912...  0.0670 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1599...  Training loss: 7.1914...  0.0600 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1600...  Training loss: 7.1906...  0.0520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1601...  Training loss: 7.1924...  0.0400 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1602...  Training loss: 7.1913...  0.0420 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1603...  Training loss: 7.1914...  0.0420 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1604...  Training loss: 7.1922...  0.0430 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1605...  Training loss: 7.1904...  0.0440 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1606...  Training loss: 7.1906...  0.0420 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1607...  Training loss: 7.1909...  0.0540 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1608...  Training loss: 7.1908...  0.0390 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1609...  Training loss: 7.1906...  0.0410 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1610...  Training loss: 7.1911...  0.0390 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1611...  Training loss: 7.1913...  0.0480 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1612...  Training loss: 7.1927...  0.0520 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1613...  Training loss: 7.1892...  0.0560 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1614...  Training loss: 7.1918...  0.0420 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1615...  Training loss: 7.1894...  0.0400 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1616...  Training loss: 7.1900...  0.0440 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1617...  Training loss: 7.1904...  0.0560 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1618...  Training loss: 7.1912...  0.0500 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1619...  Training loss: 7.1912...  0.0480 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1620...  Training loss: 7.1911...  0.0580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1621...  Training loss: 7.1915...  0.0470 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1622...  Training loss: 7.1908...  0.0510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1623...  Training loss: 7.1918...  0.0410 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1624...  Training loss: 7.1897...  0.0400 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1625...  Training loss: 7.1907...  0.0420 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1626...  Training loss: 7.1923...  0.0380 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1627...  Training loss: 7.1904...  0.0420 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1628...  Training loss: 7.1928...  0.0450 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1629...  Training loss: 7.1927...  0.0610 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1630...  Training loss: 7.1924...  0.0400 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1631...  Training loss: 7.1897...  0.0610 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1632...  Training loss: 7.1916...  0.0430 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1633...  Training loss: 7.1909...  0.0460 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1634...  Training loss: 7.1908...  0.0550 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1635...  Training loss: 7.1911...  0.0600 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1636...  Training loss: 7.1904...  0.0410 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1637...  Training loss: 7.1917...  0.0590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 1638...  Training loss: 7.1913...  0.0550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1639...  Training loss: 7.1923...  0.0520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1640...  Training loss: 7.1911...  0.0520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1641...  Training loss: 7.1926...  0.0380 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1642...  Training loss: 7.1908...  0.0430 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1643...  Training loss: 7.1914...  0.0360 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1644...  Training loss: 7.1919...  0.0390 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1645...  Training loss: 7.1920...  0.0410 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1646...  Training loss: 7.1901...  0.0370 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1647...  Training loss: 7.1923...  0.0670 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1648...  Training loss: 7.1905...  0.0560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1649...  Training loss: 7.1926...  0.0380 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1650...  Training loss: 7.1911...  0.0430 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1651...  Training loss: 7.1913...  0.0650 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1652...  Training loss: 7.1922...  0.0610 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1653...  Training loss: 7.1900...  0.0520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1654...  Training loss: 7.1925...  0.0540 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1655...  Training loss: 7.1915...  0.0430 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1656...  Training loss: 7.1922...  0.0380 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1657...  Training loss: 7.1900...  0.0400 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1658...  Training loss: 7.1912...  0.0460 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1659...  Training loss: 7.1901...  0.0440 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1660...  Training loss: 7.1921...  0.0600 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1661...  Training loss: 7.1911...  0.0380 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1662...  Training loss: 7.1918...  0.0510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1663...  Training loss: 7.1909...  0.0360 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1664...  Training loss: 7.1912...  0.0390 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1665...  Training loss: 7.1911...  0.0380 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1666...  Training loss: 7.1905...  0.0400 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1667...  Training loss: 7.1924...  0.0400 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1668...  Training loss: 7.1910...  0.0480 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1669...  Training loss: 7.1906...  0.0400 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1670...  Training loss: 7.1918...  0.0540 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1671...  Training loss: 7.1920...  0.0540 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1672...  Training loss: 7.1899...  0.0380 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1673...  Training loss: 7.1918...  0.0470 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1674...  Training loss: 7.1904...  0.0380 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1675...  Training loss: 7.1910...  0.0380 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1676...  Training loss: 7.1904...  0.0410 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1677...  Training loss: 7.1910...  0.0500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1678...  Training loss: 7.1913...  0.0530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1679...  Training loss: 7.1897...  0.0490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1680...  Training loss: 7.1910...  0.0430 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1681...  Training loss: 7.1919...  0.0360 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1682...  Training loss: 7.1918...  0.0420 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1683...  Training loss: 7.1907...  0.0400 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1684...  Training loss: 7.1910...  0.0350 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1685...  Training loss: 7.1904...  0.0390 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1686...  Training loss: 7.1913...  0.0360 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1687...  Training loss: 7.1902...  0.0550 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1688...  Training loss: 7.1917...  0.0420 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1689...  Training loss: 7.1910...  0.0510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1690...  Training loss: 7.1922...  0.0500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1691...  Training loss: 7.1908...  0.0540 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1692...  Training loss: 7.1894...  0.0490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1693...  Training loss: 7.1901...  0.0450 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1694...  Training loss: 7.1911...  0.0460 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1695...  Training loss: 7.1894...  0.0540 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1696...  Training loss: 7.1920...  0.0440 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1697...  Training loss: 7.1911...  0.0450 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1698...  Training loss: 7.1907...  0.0520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1699...  Training loss: 7.1892...  0.0430 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1700...  Training loss: 7.1907...  0.0440 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1701...  Training loss: 7.1898...  0.0360 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1702...  Training loss: 7.1914...  0.0360 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1703...  Training loss: 7.1916...  0.0400 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1704...  Training loss: 7.1917...  0.0370 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1705...  Training loss: 7.1914...  0.0360 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1706...  Training loss: 7.1904...  0.0410 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1707...  Training loss: 7.1909...  0.0480 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1708...  Training loss: 7.1913...  0.0490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1709...  Training loss: 7.1903...  0.0470 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1710...  Training loss: 7.1913...  0.0490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1711...  Training loss: 7.1903...  0.0460 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1712...  Training loss: 7.1907...  0.0610 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1713...  Training loss: 7.1929...  0.0450 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1714...  Training loss: 7.1922...  0.0390 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1715...  Training loss: 7.1901...  0.0510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1716...  Training loss: 7.1911...  0.0430 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1717...  Training loss: 7.1906...  0.0450 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1718...  Training loss: 7.1930...  0.0470 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1719...  Training loss: 7.1899...  0.0450 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1720...  Training loss: 7.1914...  0.0650 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1721...  Training loss: 7.1910...  0.0390 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1722...  Training loss: 7.1889...  0.0400 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1723...  Training loss: 7.1904...  0.0440 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1724...  Training loss: 7.1911...  0.0450 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1725...  Training loss: 7.1915...  0.0460 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1726...  Training loss: 7.1917...  0.0470 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1727...  Training loss: 7.1910...  0.0620 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1728...  Training loss: 7.1907...  0.0660 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1729...  Training loss: 7.1918...  0.0470 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1730...  Training loss: 7.1901...  0.0520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1731...  Training loss: 7.1923...  0.0590 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1732...  Training loss: 7.1904...  0.0570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1733...  Training loss: 7.1898...  0.0570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1734...  Training loss: 7.1915...  0.0690 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1735...  Training loss: 7.1903...  0.0640 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1736...  Training loss: 7.1908...  0.0470 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1737...  Training loss: 7.1908...  0.0530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1738...  Training loss: 7.1901...  0.0600 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1739...  Training loss: 7.1915...  0.0480 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1740...  Training loss: 7.1907...  0.0510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1741...  Training loss: 7.1892...  0.0450 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1742...  Training loss: 7.1918...  0.0380 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1743...  Training loss: 7.1912...  0.0420 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1744...  Training loss: 7.1890...  0.0390 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1745...  Training loss: 7.1910...  0.0440 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1746...  Training loss: 7.1903...  0.0420 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1747...  Training loss: 7.1916...  0.0670 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1748...  Training loss: 7.1891...  0.0460 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1749...  Training loss: 7.1907...  0.0490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1750...  Training loss: 7.1903...  0.0490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1751...  Training loss: 7.1891...  0.0450 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1752...  Training loss: 7.1915...  0.0530 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1753...  Training loss: 7.1898...  0.0390 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1754...  Training loss: 7.1919...  0.0560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 1755...  Training loss: 7.1906...  0.0450 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1756...  Training loss: 7.1915...  0.0500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1757...  Training loss: 7.1911...  0.0410 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1758...  Training loss: 7.1930...  0.0390 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1759...  Training loss: 7.1904...  0.0460 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1760...  Training loss: 7.1900...  0.0550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1761...  Training loss: 7.1915...  0.0540 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1762...  Training loss: 7.1912...  0.0480 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1763...  Training loss: 7.1914...  0.0670 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1764...  Training loss: 7.1940...  0.0490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1765...  Training loss: 7.1899...  0.0740 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1766...  Training loss: 7.1926...  0.0520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1767...  Training loss: 7.1906...  0.0560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1768...  Training loss: 7.1921...  0.0710 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1769...  Training loss: 7.1919...  0.0690 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1770...  Training loss: 7.1902...  0.0560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1771...  Training loss: 7.1897...  0.0510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1772...  Training loss: 7.1904...  0.0690 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1773...  Training loss: 7.1920...  0.0680 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1774...  Training loss: 7.1903...  0.0690 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1775...  Training loss: 7.1910...  0.0490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1776...  Training loss: 7.1905...  0.0630 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1777...  Training loss: 7.1913...  0.0560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1778...  Training loss: 7.1910...  0.0520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1779...  Training loss: 7.1909...  0.0710 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1780...  Training loss: 7.1908...  0.0580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1781...  Training loss: 7.1900...  0.0460 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1782...  Training loss: 7.1903...  0.0480 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1783...  Training loss: 7.1910...  0.0410 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1784...  Training loss: 7.1906...  0.0540 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1785...  Training loss: 7.1901...  0.0570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1786...  Training loss: 7.1907...  0.0520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1787...  Training loss: 7.1900...  0.0560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1788...  Training loss: 7.1926...  0.0490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1789...  Training loss: 7.1897...  0.0420 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1790...  Training loss: 7.1918...  0.0550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1791...  Training loss: 7.1897...  0.0410 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1792...  Training loss: 7.1914...  0.0670 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1793...  Training loss: 7.1908...  0.0500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1794...  Training loss: 7.1893...  0.0580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1795...  Training loss: 7.1908...  0.0510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1796...  Training loss: 7.1900...  0.0590 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1797...  Training loss: 7.1911...  0.0390 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1798...  Training loss: 7.1901...  0.0650 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1799...  Training loss: 7.1914...  0.0390 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1800...  Training loss: 7.1922...  0.0550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1801...  Training loss: 7.1911...  0.0390 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1802...  Training loss: 7.1903...  0.0410 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1803...  Training loss: 7.1908...  0.0420 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1804...  Training loss: 7.1903...  0.0420 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1805...  Training loss: 7.1918...  0.0430 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1806...  Training loss: 7.1903...  0.0520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1807...  Training loss: 7.1899...  0.0540 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1808...  Training loss: 7.1900...  0.0640 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1809...  Training loss: 7.1901...  0.0460 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1810...  Training loss: 7.1900...  0.0580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1811...  Training loss: 7.1910...  0.0450 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1812...  Training loss: 7.1894...  0.0600 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1813...  Training loss: 7.1930...  0.0500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1814...  Training loss: 7.1910...  0.0610 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1815...  Training loss: 7.1906...  0.0620 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1816...  Training loss: 7.1884...  0.0500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1817...  Training loss: 7.1908...  0.0570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1818...  Training loss: 7.1901...  0.0610 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1819...  Training loss: 7.1914...  0.0690 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1820...  Training loss: 7.1911...  0.0590 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1821...  Training loss: 7.1909...  0.0440 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1822...  Training loss: 7.1928...  0.0390 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1823...  Training loss: 7.1900...  0.0420 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1824...  Training loss: 7.1902...  0.0420 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1825...  Training loss: 7.1910...  0.0370 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1826...  Training loss: 7.1892...  0.0550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1827...  Training loss: 7.1901...  0.0740 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1828...  Training loss: 7.1901...  0.0480 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1829...  Training loss: 7.1893...  0.0490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1830...  Training loss: 7.1913...  0.0600 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1831...  Training loss: 7.1910...  0.0580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1832...  Training loss: 7.1906...  0.0440 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1833...  Training loss: 7.1892...  0.0490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1834...  Training loss: 7.1898...  0.0450 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1835...  Training loss: 7.1922...  0.0510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1836...  Training loss: 7.1896...  0.0520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1837...  Training loss: 7.1905...  0.0430 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1838...  Training loss: 7.1907...  0.0420 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1839...  Training loss: 7.1894...  0.0420 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1840...  Training loss: 7.1895...  0.0520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1841...  Training loss: 7.1913...  0.0420 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1842...  Training loss: 7.1896...  0.0400 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1843...  Training loss: 7.1904...  0.0460 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1844...  Training loss: 7.1897...  0.0500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1845...  Training loss: 7.1910...  0.0420 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1846...  Training loss: 7.1905...  0.0470 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1847...  Training loss: 7.1909...  0.0760 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1848...  Training loss: 7.1909...  0.0460 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1849...  Training loss: 7.1902...  0.0590 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1850...  Training loss: 7.1903...  0.0470 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1851...  Training loss: 7.1894...  0.0660 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1852...  Training loss: 7.1906...  0.0480 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1853...  Training loss: 7.1910...  0.0610 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1854...  Training loss: 7.1885...  0.0600 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1855...  Training loss: 7.1916...  0.0430 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1856...  Training loss: 7.1909...  0.0520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1857...  Training loss: 7.1909...  0.0470 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1858...  Training loss: 7.1900...  0.0860 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1859...  Training loss: 7.1902...  0.0670 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1860...  Training loss: 7.1906...  0.0460 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1861...  Training loss: 7.1896...  0.0380 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1862...  Training loss: 7.1907...  0.0370 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1863...  Training loss: 7.1915...  0.0480 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1864...  Training loss: 7.1912...  0.0410 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1865...  Training loss: 7.1888...  0.0380 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1866...  Training loss: 7.1916...  0.0370 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1867...  Training loss: 7.1903...  0.0670 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1868...  Training loss: 7.1895...  0.0770 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1869...  Training loss: 7.1886...  0.0790 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1870...  Training loss: 7.1879...  0.0680 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1871...  Training loss: 7.1908...  0.0500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 1872...  Training loss: 7.1911...  0.0420 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1873...  Training loss: 7.1919...  0.0370 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1874...  Training loss: 7.1898...  0.0440 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1875...  Training loss: 7.1910...  0.0570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1876...  Training loss: 7.1906...  0.0490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1877...  Training loss: 7.1902...  0.0650 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1878...  Training loss: 7.1907...  0.0600 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1879...  Training loss: 7.1914...  0.0440 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1880...  Training loss: 7.1912...  0.0570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1881...  Training loss: 7.1934...  0.0450 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1882...  Training loss: 7.1896...  0.0390 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1883...  Training loss: 7.1940...  0.0380 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1884...  Training loss: 7.1919...  0.0370 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1885...  Training loss: 7.1900...  0.0410 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1886...  Training loss: 7.1910...  0.0400 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1887...  Training loss: 7.1914...  0.0570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1888...  Training loss: 7.1908...  0.0530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1889...  Training loss: 7.1909...  0.0560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1890...  Training loss: 7.1903...  0.0390 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1891...  Training loss: 7.1905...  0.0460 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1892...  Training loss: 7.1896...  0.0390 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1893...  Training loss: 7.1900...  0.0620 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1894...  Training loss: 7.1913...  0.0610 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1895...  Training loss: 7.1904...  0.0470 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1896...  Training loss: 7.1890...  0.0390 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1897...  Training loss: 7.1896...  0.0440 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1898...  Training loss: 7.1905...  0.0540 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1899...  Training loss: 7.1909...  0.0370 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1900...  Training loss: 7.1898...  0.0460 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1901...  Training loss: 7.1902...  0.0440 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1902...  Training loss: 7.1894...  0.0410 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1903...  Training loss: 7.1915...  0.0370 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1904...  Training loss: 7.1904...  0.0390 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1905...  Training loss: 7.1904...  0.0410 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1906...  Training loss: 7.1897...  0.0450 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1907...  Training loss: 7.1911...  0.0580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1908...  Training loss: 7.1886...  0.0510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1909...  Training loss: 7.1905...  0.0430 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1910...  Training loss: 7.1899...  0.0450 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1911...  Training loss: 7.1892...  0.0560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1912...  Training loss: 7.1898...  0.0440 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1913...  Training loss: 7.1880...  0.0590 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1914...  Training loss: 7.1924...  0.0400 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1915...  Training loss: 7.1906...  0.0600 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1916...  Training loss: 7.1898...  0.0490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1917...  Training loss: 7.1928...  0.0650 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1918...  Training loss: 7.1901...  0.0600 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1919...  Training loss: 7.1896...  0.0490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1920...  Training loss: 7.1904...  0.0480 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1921...  Training loss: 7.1890...  0.0380 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1922...  Training loss: 7.1898...  0.0500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1923...  Training loss: 7.1893...  0.0470 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1924...  Training loss: 7.1890...  0.0380 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1925...  Training loss: 7.1887...  0.0400 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1926...  Training loss: 7.1899...  0.0430 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1927...  Training loss: 7.1903...  0.0500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1928...  Training loss: 7.1898...  0.0400 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1929...  Training loss: 7.1881...  0.0510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1930...  Training loss: 7.1907...  0.0630 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1931...  Training loss: 7.1922...  0.0630 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1932...  Training loss: 7.1898...  0.0610 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1933...  Training loss: 7.1899...  0.0440 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1934...  Training loss: 7.1902...  0.0460 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1935...  Training loss: 7.1892...  0.0470 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1936...  Training loss: 7.1906...  0.0420 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1937...  Training loss: 7.1901...  0.0740 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1938...  Training loss: 7.1901...  0.0410 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1939...  Training loss: 7.1915...  0.0400 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1940...  Training loss: 7.1899...  0.0410 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1941...  Training loss: 7.1884...  0.0490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1942...  Training loss: 7.1907...  0.0440 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1943...  Training loss: 7.1901...  0.0390 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1944...  Training loss: 7.1900...  0.0450 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1945...  Training loss: 7.1893...  0.0410 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1946...  Training loss: 7.1894...  0.0540 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1947...  Training loss: 7.1905...  0.0690 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1948...  Training loss: 7.1915...  0.0520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1949...  Training loss: 7.1897...  0.0580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1950...  Training loss: 7.1892...  0.0810 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1951...  Training loss: 7.1884...  0.0660 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1952...  Training loss: 7.1902...  0.0550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1953...  Training loss: 7.1893...  0.0550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1954...  Training loss: 7.1901...  0.0640 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1955...  Training loss: 7.1912...  0.0460 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1956...  Training loss: 7.1885...  0.0610 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1957...  Training loss: 7.1897...  0.0380 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1958...  Training loss: 7.1917...  0.0480 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1959...  Training loss: 7.1909...  0.0680 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1960...  Training loss: 7.1910...  0.0390 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1961...  Training loss: 7.1906...  0.0380 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1962...  Training loss: 7.1899...  0.0400 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1963...  Training loss: 7.1915...  0.0370 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1964...  Training loss: 7.1907...  0.0410 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1965...  Training loss: 7.1894...  0.0400 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1966...  Training loss: 7.1897...  0.0400 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1967...  Training loss: 7.1890...  0.0580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1968...  Training loss: 7.1896...  0.0560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1969...  Training loss: 7.1904...  0.0400 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1970...  Training loss: 7.1909...  0.0490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1971...  Training loss: 7.1895...  0.0430 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1972...  Training loss: 7.1909...  0.0420 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1973...  Training loss: 7.1902...  0.0580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1974...  Training loss: 7.1905...  0.0540 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1975...  Training loss: 7.1885...  0.0530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1976...  Training loss: 7.1903...  0.0480 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1977...  Training loss: 7.1918...  0.0520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1978...  Training loss: 7.1884...  0.0460 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1979...  Training loss: 7.1920...  0.0490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1980...  Training loss: 7.1886...  0.0470 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1981...  Training loss: 7.1899...  0.0400 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1982...  Training loss: 7.1880...  0.0470 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1983...  Training loss: 7.1900...  0.0410 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1984...  Training loss: 7.1883...  0.0370 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1985...  Training loss: 7.1900...  0.0410 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1986...  Training loss: 7.1898...  0.0520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1987...  Training loss: 7.1874...  0.0710 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1988...  Training loss: 7.1894...  0.0450 sec/batch\n",
      "Epoch: 17/20...  Training Step: 1989...  Training loss: 7.1894...  0.0390 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1990...  Training loss: 7.1910...  0.0580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1991...  Training loss: 7.1903...  0.0480 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1992...  Training loss: 7.1898...  0.0570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1993...  Training loss: 7.1898...  0.0520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1994...  Training loss: 7.1898...  0.0440 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1995...  Training loss: 7.1907...  0.0550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1996...  Training loss: 7.1899...  0.0470 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1997...  Training loss: 7.1898...  0.0440 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1998...  Training loss: 7.1929...  0.0390 sec/batch\n",
      "Epoch: 18/20...  Training Step: 1999...  Training loss: 7.1909...  0.0470 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2000...  Training loss: 7.1922...  0.0410 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2001...  Training loss: 7.1914...  0.0360 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2002...  Training loss: 7.1911...  0.0370 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2003...  Training loss: 7.1893...  0.0460 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2004...  Training loss: 7.1884...  0.0360 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2005...  Training loss: 7.1885...  0.0420 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2006...  Training loss: 7.1902...  0.0590 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2007...  Training loss: 7.1904...  0.0830 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2008...  Training loss: 7.1891...  0.0470 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2009...  Training loss: 7.1882...  0.0460 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2010...  Training loss: 7.1893...  0.0570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2011...  Training loss: 7.1899...  0.0810 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2012...  Training loss: 7.1915...  0.0450 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2013...  Training loss: 7.1889...  0.0600 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2014...  Training loss: 7.1890...  0.0580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2015...  Training loss: 7.1900...  0.0420 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2016...  Training loss: 7.1895...  0.0480 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2017...  Training loss: 7.1886...  0.0480 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2018...  Training loss: 7.1901...  0.0440 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2019...  Training loss: 7.1884...  0.0410 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2020...  Training loss: 7.1906...  0.0490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2021...  Training loss: 7.1908...  0.0400 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2022...  Training loss: 7.1901...  0.0400 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2023...  Training loss: 7.1899...  0.0360 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2024...  Training loss: 7.1910...  0.0450 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2025...  Training loss: 7.1897...  0.0400 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2026...  Training loss: 7.1902...  0.0400 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2027...  Training loss: 7.1884...  0.0570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2028...  Training loss: 7.1891...  0.0590 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2029...  Training loss: 7.1909...  0.0430 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2030...  Training loss: 7.1870...  0.0440 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2031...  Training loss: 7.1905...  0.0390 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2032...  Training loss: 7.1880...  0.0390 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2033...  Training loss: 7.1906...  0.0450 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2034...  Training loss: 7.1902...  0.0530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2035...  Training loss: 7.1885...  0.0480 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2036...  Training loss: 7.1887...  0.0410 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2037...  Training loss: 7.1896...  0.0470 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2038...  Training loss: 7.1885...  0.0440 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2039...  Training loss: 7.1901...  0.0490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2040...  Training loss: 7.1891...  0.0450 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2041...  Training loss: 7.1901...  0.0380 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2042...  Training loss: 7.1891...  0.0410 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2043...  Training loss: 7.1890...  0.0390 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2044...  Training loss: 7.1882...  0.0370 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2045...  Training loss: 7.1900...  0.0380 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2046...  Training loss: 7.1887...  0.0410 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2047...  Training loss: 7.1906...  0.0470 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2048...  Training loss: 7.1907...  0.0450 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2049...  Training loss: 7.1879...  0.0410 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2050...  Training loss: 7.1899...  0.0480 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2051...  Training loss: 7.1876...  0.0400 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2052...  Training loss: 7.1889...  0.0400 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2053...  Training loss: 7.1894...  0.0550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2054...  Training loss: 7.1895...  0.0500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2055...  Training loss: 7.1899...  0.0390 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2056...  Training loss: 7.1920...  0.0590 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2057...  Training loss: 7.1908...  0.0450 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2058...  Training loss: 7.1881...  0.0480 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2059...  Training loss: 7.1911...  0.0370 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2060...  Training loss: 7.1898...  0.0520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2061...  Training loss: 7.1908...  0.0390 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2062...  Training loss: 7.1894...  0.0420 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2063...  Training loss: 7.1887...  0.0370 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2064...  Training loss: 7.1894...  0.0380 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2065...  Training loss: 7.1919...  0.0420 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2066...  Training loss: 7.1883...  0.0390 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2067...  Training loss: 7.1909...  0.0540 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2068...  Training loss: 7.1884...  0.0450 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2069...  Training loss: 7.1899...  0.0630 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2070...  Training loss: 7.1890...  0.0620 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2071...  Training loss: 7.1884...  0.0590 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2072...  Training loss: 7.1913...  0.0560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2073...  Training loss: 7.1876...  0.0740 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2074...  Training loss: 7.1884...  0.0710 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2075...  Training loss: 7.1909...  0.0760 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2076...  Training loss: 7.1883...  0.0460 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2077...  Training loss: 7.1884...  0.0560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2078...  Training loss: 7.1900...  0.0460 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2079...  Training loss: 7.1894...  0.0750 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2080...  Training loss: 7.1905...  0.0730 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2081...  Training loss: 7.1889...  0.0380 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2082...  Training loss: 7.1890...  0.0470 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2083...  Training loss: 7.1865...  0.0450 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2084...  Training loss: 7.1882...  0.0460 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2085...  Training loss: 7.1885...  0.0460 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2086...  Training loss: 7.1894...  0.0580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2087...  Training loss: 7.1911...  0.0700 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2088...  Training loss: 7.1881...  0.0550 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2089...  Training loss: 7.1905...  0.0500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2090...  Training loss: 7.1904...  0.0500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2091...  Training loss: 7.1895...  0.0610 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2092...  Training loss: 7.1893...  0.0510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2093...  Training loss: 7.1887...  0.0520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2094...  Training loss: 7.1920...  0.0500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2095...  Training loss: 7.1883...  0.0580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2096...  Training loss: 7.1910...  0.0680 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2097...  Training loss: 7.1900...  0.0420 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2098...  Training loss: 7.1897...  0.0500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2099...  Training loss: 7.1881...  0.0510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2100...  Training loss: 7.1906...  0.0710 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2101...  Training loss: 7.1873...  0.0380 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2102...  Training loss: 7.1893...  0.0470 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2103...  Training loss: 7.1898...  0.0380 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2104...  Training loss: 7.1888...  0.0500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2105...  Training loss: 7.1877...  0.0380 sec/batch\n",
      "Epoch: 18/20...  Training Step: 2106...  Training loss: 7.1894...  0.0390 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2107...  Training loss: 7.1910...  0.0600 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2108...  Training loss: 7.1898...  0.0680 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2109...  Training loss: 7.1921...  0.0510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2110...  Training loss: 7.1896...  0.0560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2111...  Training loss: 7.1896...  0.0500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2112...  Training loss: 7.1903...  0.0490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2113...  Training loss: 7.1906...  0.0480 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2114...  Training loss: 7.1906...  0.0420 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2115...  Training loss: 7.1915...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2116...  Training loss: 7.1887...  0.0380 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2117...  Training loss: 7.1933...  0.0680 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2118...  Training loss: 7.1903...  0.0560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2119...  Training loss: 7.1922...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2120...  Training loss: 7.1889...  0.0420 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2121...  Training loss: 7.1896...  0.0420 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2122...  Training loss: 7.1890...  0.0410 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2123...  Training loss: 7.1903...  0.0420 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2124...  Training loss: 7.1889...  0.0430 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2125...  Training loss: 7.1876...  0.0380 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2126...  Training loss: 7.1883...  0.0410 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2127...  Training loss: 7.1894...  0.0550 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2128...  Training loss: 7.1916...  0.0520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2129...  Training loss: 7.1893...  0.0660 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2130...  Training loss: 7.1879...  0.0570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2131...  Training loss: 7.1883...  0.0430 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2132...  Training loss: 7.1899...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2133...  Training loss: 7.1897...  0.0420 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2134...  Training loss: 7.1905...  0.0470 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2135...  Training loss: 7.1893...  0.0440 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2136...  Training loss: 7.1897...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2137...  Training loss: 7.1881...  0.0550 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2138...  Training loss: 7.1901...  0.0500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2139...  Training loss: 7.1907...  0.0460 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2140...  Training loss: 7.1892...  0.0570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2141...  Training loss: 7.1920...  0.0370 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2142...  Training loss: 7.1889...  0.0390 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2143...  Training loss: 7.1905...  0.0390 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2144...  Training loss: 7.1892...  0.0430 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2145...  Training loss: 7.1891...  0.0380 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2146...  Training loss: 7.1900...  0.0460 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2147...  Training loss: 7.1868...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2148...  Training loss: 7.1897...  0.0560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2149...  Training loss: 7.1879...  0.0480 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2150...  Training loss: 7.1910...  0.0560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2151...  Training loss: 7.1900...  0.0480 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2152...  Training loss: 7.1887...  0.0490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2153...  Training loss: 7.1890...  0.0780 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2154...  Training loss: 7.1897...  0.0470 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2155...  Training loss: 7.1877...  0.0640 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2156...  Training loss: 7.1905...  0.0410 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2157...  Training loss: 7.1890...  0.0480 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2158...  Training loss: 7.1887...  0.0550 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2159...  Training loss: 7.1885...  0.0520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2160...  Training loss: 7.1876...  0.0510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2161...  Training loss: 7.1894...  0.0470 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2162...  Training loss: 7.1892...  0.0430 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2163...  Training loss: 7.1878...  0.0430 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2164...  Training loss: 7.1880...  0.0410 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2165...  Training loss: 7.1893...  0.0400 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2166...  Training loss: 7.1882...  0.0480 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2167...  Training loss: 7.1880...  0.0540 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2168...  Training loss: 7.1876...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2169...  Training loss: 7.1880...  0.0560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2170...  Training loss: 7.1904...  0.0510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2171...  Training loss: 7.1902...  0.0520 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2172...  Training loss: 7.1901...  0.0480 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2173...  Training loss: 7.1922...  0.0460 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2174...  Training loss: 7.1896...  0.0610 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2175...  Training loss: 7.1903...  0.0460 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2176...  Training loss: 7.1889...  0.0670 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2177...  Training loss: 7.1882...  0.0500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2178...  Training loss: 7.1883...  0.0580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2179...  Training loss: 7.1883...  0.0620 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2180...  Training loss: 7.1891...  0.0600 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2181...  Training loss: 7.1883...  0.0400 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2182...  Training loss: 7.1906...  0.0370 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2183...  Training loss: 7.1899...  0.0440 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2184...  Training loss: 7.1883...  0.0400 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2185...  Training loss: 7.1897...  0.0490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2186...  Training loss: 7.1906...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2187...  Training loss: 7.1873...  0.0470 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2188...  Training loss: 7.1897...  0.0600 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2189...  Training loss: 7.1902...  0.0560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2190...  Training loss: 7.1867...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2191...  Training loss: 7.1884...  0.0500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2192...  Training loss: 7.1916...  0.0570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2193...  Training loss: 7.1887...  0.0380 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2194...  Training loss: 7.1883...  0.0510 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2195...  Training loss: 7.1877...  0.0550 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2196...  Training loss: 7.1905...  0.0480 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2197...  Training loss: 7.1901...  0.0660 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2198...  Training loss: 7.1884...  0.0540 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2199...  Training loss: 7.1903...  0.0420 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2200...  Training loss: 7.1875...  0.0600 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2201...  Training loss: 7.1883...  0.0470 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2202...  Training loss: 7.1874...  0.0390 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2203...  Training loss: 7.1900...  0.0400 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2204...  Training loss: 7.1889...  0.0380 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2205...  Training loss: 7.1890...  0.0490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2206...  Training loss: 7.1892...  0.0410 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2207...  Training loss: 7.1903...  0.0530 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2208...  Training loss: 7.1906...  0.0450 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2209...  Training loss: 7.1886...  0.0500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2210...  Training loss: 7.1901...  0.0470 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2211...  Training loss: 7.1908...  0.0420 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2212...  Training loss: 7.1867...  0.0490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2213...  Training loss: 7.1900...  0.0590 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2214...  Training loss: 7.1900...  0.0430 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2215...  Training loss: 7.1902...  0.0590 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2216...  Training loss: 7.1874...  0.0420 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2217...  Training loss: 7.1902...  0.0470 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2218...  Training loss: 7.1881...  0.0440 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2219...  Training loss: 7.1880...  0.0580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2220...  Training loss: 7.1885...  0.0420 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2221...  Training loss: 7.1872...  0.0400 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2222...  Training loss: 7.1898...  0.0380 sec/batch\n",
      "Epoch: 19/20...  Training Step: 2223...  Training loss: 7.1871...  0.0380 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2224...  Training loss: 7.1907...  0.0440 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2225...  Training loss: 7.1904...  0.0370 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2226...  Training loss: 7.1915...  0.0390 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2227...  Training loss: 7.1891...  0.0600 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2228...  Training loss: 7.1878...  0.0480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2229...  Training loss: 7.1897...  0.0460 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2230...  Training loss: 7.1901...  0.0520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2231...  Training loss: 7.1899...  0.0500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2232...  Training loss: 7.1915...  0.0590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2233...  Training loss: 7.1880...  0.0430 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2234...  Training loss: 7.1917...  0.0450 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2235...  Training loss: 7.1900...  0.0540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2236...  Training loss: 7.1916...  0.0540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2237...  Training loss: 7.1904...  0.0450 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2238...  Training loss: 7.1893...  0.0370 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2239...  Training loss: 7.1909...  0.0520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2240...  Training loss: 7.1895...  0.0380 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2241...  Training loss: 7.1908...  0.0370 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2242...  Training loss: 7.1881...  0.0410 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2243...  Training loss: 7.1878...  0.0350 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2244...  Training loss: 7.1895...  0.0370 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2245...  Training loss: 7.1907...  0.0370 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2246...  Training loss: 7.1898...  0.0370 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2247...  Training loss: 7.1885...  0.0460 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2248...  Training loss: 7.1884...  0.0560 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2249...  Training loss: 7.1889...  0.0440 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2250...  Training loss: 7.1879...  0.0510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2251...  Training loss: 7.1880...  0.0430 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2252...  Training loss: 7.1904...  0.0530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2253...  Training loss: 7.1870...  0.0480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2254...  Training loss: 7.1871...  0.0540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2255...  Training loss: 7.1895...  0.0570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2256...  Training loss: 7.1893...  0.0440 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2257...  Training loss: 7.1871...  0.0570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2258...  Training loss: 7.1902...  0.0490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2259...  Training loss: 7.1888...  0.0400 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2260...  Training loss: 7.1891...  0.0430 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2261...  Training loss: 7.1878...  0.0390 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2262...  Training loss: 7.1890...  0.0470 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2263...  Training loss: 7.1887...  0.0410 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2264...  Training loss: 7.1863...  0.0370 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2265...  Training loss: 7.1903...  0.0380 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2266...  Training loss: 7.1879...  0.0400 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2267...  Training loss: 7.1899...  0.0580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2268...  Training loss: 7.1880...  0.0410 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2269...  Training loss: 7.1887...  0.0420 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2270...  Training loss: 7.1883...  0.0570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2271...  Training loss: 7.1885...  0.0410 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2272...  Training loss: 7.1872...  0.0660 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2273...  Training loss: 7.1877...  0.0570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2274...  Training loss: 7.1882...  0.0590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2275...  Training loss: 7.1896...  0.0400 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2276...  Training loss: 7.1877...  0.0390 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2277...  Training loss: 7.1877...  0.0390 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2278...  Training loss: 7.1866...  0.0440 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2279...  Training loss: 7.1888...  0.0540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2280...  Training loss: 7.1875...  0.0390 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2281...  Training loss: 7.1902...  0.0420 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2282...  Training loss: 7.1885...  0.0400 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2283...  Training loss: 7.1873...  0.0370 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2284...  Training loss: 7.1876...  0.0380 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2285...  Training loss: 7.1885...  0.0380 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2286...  Training loss: 7.1900...  0.0360 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2287...  Training loss: 7.1877...  0.0580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2288...  Training loss: 7.1904...  0.0430 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2289...  Training loss: 7.1905...  0.0370 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2290...  Training loss: 7.1895...  0.0400 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2291...  Training loss: 7.1894...  0.0430 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2292...  Training loss: 7.1877...  0.0440 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2293...  Training loss: 7.1890...  0.0440 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2294...  Training loss: 7.1884...  0.0630 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2295...  Training loss: 7.1896...  0.0390 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2296...  Training loss: 7.1882...  0.0390 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2297...  Training loss: 7.1891...  0.0460 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2298...  Training loss: 7.1877...  0.0490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2299...  Training loss: 7.1873...  0.0460 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2300...  Training loss: 7.1873...  0.0440 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2301...  Training loss: 7.1892...  0.0410 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2302...  Training loss: 7.1885...  0.0470 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2303...  Training loss: 7.1905...  0.0430 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2304...  Training loss: 7.1869...  0.0440 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2305...  Training loss: 7.1894...  0.0410 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2306...  Training loss: 7.1894...  0.0570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2307...  Training loss: 7.1866...  0.0490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2308...  Training loss: 7.1886...  0.0480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2309...  Training loss: 7.1901...  0.0620 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2310...  Training loss: 7.1889...  0.0510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2311...  Training loss: 7.1885...  0.0480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2312...  Training loss: 7.1875...  0.0480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2313...  Training loss: 7.1859...  0.0580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2314...  Training loss: 7.1895...  0.0730 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2315...  Training loss: 7.1885...  0.0720 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2316...  Training loss: 7.1896...  0.0430 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2317...  Training loss: 7.1872...  0.0500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2318...  Training loss: 7.1875...  0.0520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2319...  Training loss: 7.1880...  0.0580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2320...  Training loss: 7.1872...  0.0470 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2321...  Training loss: 7.1895...  0.0420 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2322...  Training loss: 7.1884...  0.0390 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2323...  Training loss: 7.1903...  0.0370 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2324...  Training loss: 7.1888...  0.0440 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2325...  Training loss: 7.1882...  0.0500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2326...  Training loss: 7.1862...  0.0650 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2327...  Training loss: 7.1875...  0.0490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2328...  Training loss: 7.1888...  0.0420 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2329...  Training loss: 7.1871...  0.0710 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2330...  Training loss: 7.1908...  0.0670 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2331...  Training loss: 7.1873...  0.0400 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2332...  Training loss: 7.1890...  0.0480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2333...  Training loss: 7.1871...  0.0460 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2334...  Training loss: 7.1891...  0.0750 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2335...  Training loss: 7.1877...  0.0470 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2336...  Training loss: 7.1876...  0.0480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2337...  Training loss: 7.1877...  0.0590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2338...  Training loss: 7.1859...  0.0500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2339...  Training loss: 7.1900...  0.0620 sec/batch\n",
      "Epoch: 20/20...  Training Step: 2340...  Training loss: 7.1893...  0.0450 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# 存放变量20，用于后来的保存\n",
    "save_every_n = 20\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "#只保留最近5个checkpoint\n",
    "saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 如果加入下面的注释 可以继续训练\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    #new_state = sess.run(model.initial_state)\n",
    "    for e in range(epochs):\n",
    "        # 训练网络\n",
    "        #设置第一次的state是模型的初始state，都是0\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        #print(new_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            #单次get_batches会获得4个序列的x 和 y\n",
    "            #计数器每次加一\n",
    "            counter += 1\n",
    "            #print('new state count',counter,new_state)\n",
    "            #记录时间\n",
    "            start = time.time()\n",
    "            #创建feed字典\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            #运行会话model.loss,model.final_state,model.optimizer\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            #记录单批运算时间\n",
    "            end = time.time()\n",
    "            #打印当前Epoch,当前训练batch对应的step，当前batch对应的loss，当前step对应的消耗\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "            \n",
    "            #每20次训练保存一次，保存到checkpoints文件夹文件名为i{计数器}_l{lstm_unit个数}\n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints\\\\i2340_l10.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2260_l10.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2280_l10.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2300_l10.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2320_l10.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2340_l10.ckpt\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=500):\n",
    "    \n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n",
      "p= [ 0.1  0.   0.2  0.7]\n",
      "p的shape (4,)\n",
      "argsort 后的结果 [1 0 2 3]\n",
      "[ 0.   0.1  0.2  0.7]\n",
      "[ 0.   0.1  0.2]\n",
      "[ 0.   0.1]\n",
      "设置排序后的数据的前2个 ,这里2个是因为top_n=2\n",
      "[ 0.          0.          0.22222222  0.77777778]\n",
      "[0 1 2 3]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "preds, vocab_size, top_n=np.array([[0.1],[0],[0.2],[0.7]]),4,2\n",
    "#np.squeeze 移除长度是1的轴，例如shape=(2,1) 变为（2,）\n",
    "print(preds.shape)\n",
    "p = np.squeeze(preds)\n",
    "print('p=' ,p)\n",
    "print('p的shape',p.shape)\n",
    "\n",
    "#argsort函数返回的是数组值从小到大的索引值\n",
    "print('argsort 后的结果',np.argsort(p))\n",
    "\n",
    "#\n",
    "print(p[np.argsort(p)] )\n",
    "print(p[np.argsort(p)[:-1]])\n",
    "print(p[np.argsort(p)[:-2]])\n",
    "\n",
    "p[np.argsort(p)[:-top_n]] = 0\n",
    "print('设置排序后的数据的前2个 ,这里2个是因为top_n=2')\n",
    "p = p / np.sum(p)\n",
    "print(p)\n",
    "\n",
    "print(np.arange(4))\n",
    "\n",
    "#挑选1个值，从随机数中挑选一个值，挑选的值来源是 np.arange(vocab_size) ，也就是[0 1 2 3]，挑选值的概率采用后面的p参数\n",
    "\n",
    "c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['月']\n",
      "build_inputs 函数中的input <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x0000000004BB3860>>\n",
      "build LSTM函数中传入的batch_size 1\n",
      "Tensor(\"one_hot:0\", shape=(1, 1, 1334), dtype=float32)\n",
      "build_output 函数中的seq_output 是否为10列 60行 ： Tensor(\"concat:0\", shape=(1, 1, 10), dtype=float32)\n",
      "build_output 函数中的 reshape seqput  Tensor(\"Reshape:0\", shape=(1, 10), dtype=float32)\n",
      "build_output 函数中的logit 是否是字典个数: Tensor(\"add:0\", shape=(1, 1334), dtype=float32)\n",
      "build_output 函数中的softmax_out Tensor(\"predictions:0\", shape=(1, 1334), dtype=float32)\n",
      "<__main__.CharRNN object at 0x00000000114D7BA8>\n",
      "月\n",
      "[[ 0.00075171  0.00075193  0.00075007 ...,  0.00075017  0.00075003\n",
      "   0.00074883]]\n",
      "1.0\n",
      "['月', '南']\n",
      "['月', '南', '入']\n",
      "['月', '南', '入', '响']\n",
      "['月', '南', '入', '响', '气']\n",
      "['月', '南', '入', '响', '气', '态']\n",
      "['月', '南', '入', '响', '气', '态', '凑']\n",
      "月南入响气态凑\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "n_samples, lstm_size,prime=5,10,\"月\"\n",
    "samples = [c for c in prime]\n",
    "print(samples)\n",
    "#传入参数sampling=True 实现单个字符的连续预测\n",
    "model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "print(model)\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint)\n",
    "    #设置初始状态全部为0\n",
    "    new_state = sess.run(model.initial_state)\n",
    "    \n",
    "    #对输入单词做循环例如输入\"仙人\",循环获得 “仙”，“人”，对应的字典编码\n",
    "    for c in prime:\n",
    "        print(c)\n",
    "        #x 设置为 [[ 0.]]\n",
    "        x = np.zeros((1, 1))\n",
    "        #当c=“人”，x=[[ 164.]]，164是人的字典编码\n",
    "        x[0,0] = vocab_to_int[c]\n",
    "        #预测是要保证keep_prob=0\n",
    "        feed = {model.inputs: x,\n",
    "                model.keep_prob: 1.,\n",
    "                model.initial_state: new_state}\n",
    "        #预测为output函数中的：tf.nn.softmax(logits, name='predictions'),\n",
    "        #例如循环到‘人’ ，从字典挑出某个词的概率。如果字典有245个文字。就有245为长度的数组，相加等于1\n",
    "        \n",
    "        preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                     feed_dict=feed)\n",
    "        print(preds)\n",
    "        print(np.sum(preds))\n",
    "        \n",
    "    #获得一个单词的下标\n",
    "    c = pick_top_n(preds, len(vocab))\n",
    "    #把单词追加到现有单词后面\n",
    "    samples.append(int_to_vocab[c])\n",
    "    print(samples)\n",
    "\n",
    "    #继续循环知道直到达到设置预测字符个数的限制\n",
    "    for i in range(n_samples):\n",
    "        x[0,0] = c\n",
    "        feed = {model.inputs: x,\n",
    "                model.keep_prob: 1.,\n",
    "                model.initial_state: new_state}\n",
    "        preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                     feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "        print(samples)\n",
    "    #以无分隔符''的方式把字符list合并\n",
    "    print(''.join(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"小\"):\n",
    "    \n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints\\\\i2340_l10.ckpt'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_inputs 函数中的input <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x0000000004BB3860>>\n",
      "build LSTM函数中传入的batch_size 1\n",
      "Tensor(\"one_hot:0\", shape=(1, 1, 1334), dtype=float32)\n",
      "build_output 函数中的seq_output 是否为10列 60行 ： Tensor(\"concat:0\", shape=(1, 1, 10), dtype=float32)\n",
      "build_output 函数中的 reshape seqput  Tensor(\"Reshape:0\", shape=(1, 10), dtype=float32)\n",
      "build_output 函数中的logit 是否是字典个数: Tensor(\"add:0\", shape=(1, 1334), dtype=float32)\n",
      "build_output 函数中的softmax_out Tensor(\"predictions:0\", shape=(1, 1334), dtype=float32)\n",
      "日不童俯悠\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint,3, lstm_size, len(vocab), prime=\"日\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
